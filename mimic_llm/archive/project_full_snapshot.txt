SECTION 1: Main project file structure (core .py files and scripts)

Project root: /home/soham_shah/mimic_llm

app_streamlit.py
dump_full_project_snapshot.py
dump_project_snapshot.py
eval.py
features.py
models.py
paths.py
prompts.py
scripts/export_three_stays_jsonl.py
scripts/export_three_stays_txt.py
scripts/filter_diagnoses_to_cohort.py
scripts/filter_discharge_to_cohort.py
scripts/filter_icustays_to_cohort.py
scripts/filter_lab_tests_to_cohort.py
scripts/filter_measurements_to_cohort.py
scripts/filter_medications_to_cohort.py
scripts/filter_outputevents_to_cohort.py
scripts/filter_patients_admissions_to_cohort.py
scripts/filter_procedureevents_to_cohort.py
scripts/filter_procedures_to_cohort.py
scripts/make_cohort_icu_250.py
scripts/make_diagnoses_clean.py
scripts/make_discharge_notes_clean.py
scripts/make_icustays_clean.py
scripts/make_lab_tests_clean.py
scripts/make_measurements_clean.py
scripts/make_medications_clean.py
scripts/make_outputevents_clean.py
scripts/make_patients_admissions_clean.py
scripts/make_procedureevents_clean.py
scripts/make_procedures_clean.py
scripts/run_single_stay_inference.py
visuals.py


--------------------------------------------------------------------------------

SECTION 2: Supporting / setup files and directories

These paths are environment/config/logging/archive/etc., not part of the final output.

DIR : /home/soham_shah/.cache
DIR : /home/soham_shah/.conda
DIR : /home/soham_shah/.config
DIR : /home/soham_shah/.dotnet
DIR : /home/soham_shah/.ipython
DIR : /home/soham_shah/.jupyter
DIR : /home/soham_shah/.local
DIR : /home/soham_shah/.mamba
DIR : /home/soham_shah/.nv
DIR : /home/soham_shah/.streamlit
DIR : /home/soham_shah/.vscode-server
DIR : /home/soham_shah/mimic_llm/__pycache__
DIR : /home/soham_shah/mimic_llm/archive
FILE: /home/soham_shah/mimic_llm/archive/check_cohort_discharge_consistency.py
FILE: /home/soham_shah/mimic_llm/archive/check_stayid_hadmid_consistency.py
FILE: /home/soham_shah/mimic_llm/archive/explore_mimic_proc_stats.py
FILE: /home/soham_shah/mimic_llm/archive/explore_proc_structure.py
FILE: /home/soham_shah/mimic_llm/archive/inspect_single_stay.py
DIR : /home/soham_shah/mimic_llm/exports
DIR : /home/soham_shah/mimic_llm/logs
DIR : /home/soham_shah/mimic_llm/notebooks
FILE: /home/soham_shah/mimic_llm/project_snapshot.txt


--------------------------------------------------------------------------------

SECTION 3: Full source code of main project files

================================================================================
FILE: app_streamlit.py
================================================================================
import os
import sys
import re

import pandas as pd
import streamlit as st

# Wire project root
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import COHORT_META_DIR

from features import (
    load_all_tables_for_stay,
    build_view_dx_proc,
    build_view_labs,
    build_view_meds,
    build_view_measurements,
    build_view_admission,
    build_view_outputs,
    build_view_procedureevents,
)

from prompts import make_prompt
from models import generate_flan, generate_meditron
from eval import compare_summaries

from visuals import (
    render_medications_visuals,
    render_measurements_visuals,
    render_outputs_visuals,
    render_labs_visuals,
    render_admission_table,
    render_diagnoses_table,
    render_hosp_procedures_table,
    render_icu_procedureevents_table,
)

VIEW_LABELS = {
    "admission": "Admission & demographics",
    "dx_proc": "Diagnosis & procedures",
    "labs": "Lab events",
    "meds": "Medications",
    "measurements": "Measurements / vitals",
    "outputs": "Output events (fluids/drains)",
    "procedureevents": "ICU bedside procedures",
    "final": "Full multi-section summary",
}

VIEW_KEYS = {v: k for k, v in VIEW_LABELS.items()}

TOKEN_LIMITS = {
    "admission": 128,
    "dx_proc": 200,
    "labs": 280,
    "meds": 280,
    "measurements": 280,
    "outputs": 200,
    "procedureevents": 200,
    "final": 800, # Needs to be long!
}

# Simple synonym map for measurement names → how they might appear in text
MEASUREMENT_SYNONYMS = {
    "Heart Rate": ["heart rate", "hr"],
    "Respiratory Rate": ["respiratory rate", "rr", "breathing rate"],
    "SpO2": ["spo2", "oxygen saturation", "o2 sat", "sat"],
    "O2 Saturation": ["oxygen saturation", "o2 sat", "spo2"],
    "Mean Arterial Pressure": ["map", "mean arterial pressure"],
    "Mean BP": ["map", "mean arterial pressure", "mean bp"],
    "Systolic BP": ["systolic bp", "sbp", "systolic blood pressure"],
    "Diastolic BP": ["diastolic bp", "dbp", "diastolic blood pressure"],
    "Temperature": ["temperature", "temp"],
    "Glucose": ["glucose", "blood sugar"],
    # Anything not in this dict will fall back to searching the raw measure_name
}

@st.cache_data
def load_cohort():
    path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    df = pd.read_parquet(path)
    # ensure int
    df["stay_id"] = df["stay_id"].astype(int)
    df["hadm_id"] = df["hadm_id"].astype(int)
    df["subject_id"] = df["subject_id"].astype(int)
    return df


def run_single_view(stay_data, view_type: str):
    """
    Run a single-view summary (one table family at a time).
    """
    discharge_text = stay_data["discharge_text"]

    if view_type == "dx_proc":
        features = build_view_dx_proc(stay_data)
    elif view_type == "labs":
        features = build_view_labs(stay_data)
    elif view_type == "meds":
        features = build_view_meds(stay_data)
    elif view_type == "measurements":
        features = build_view_measurements(stay_data)
    elif view_type == "admission":
        features = build_view_admission(stay_data)
    elif view_type == "outputs":
        features = build_view_outputs(stay_data)
    elif view_type in ("procedureevents", "procedures_icu"):
        # ICU bedside procedureevents table (not HOSP procedures)
        features = build_view_procedureevents(stay_data)
    else:
        raise ValueError(f"Unsupported single-view type: {view_type}")

    limit = TOKEN_LIMITS.get(view_type, 192)
    
    flan_prompt = make_prompt(view_type, features, model_name="flan")
    meditron_prompt = make_prompt(view_type, features, model_name="meditron")

    flan_summary = generate_flan(flan_prompt, max_new_tokens=limit)
    meditron_summary = generate_meditron(
        meditron_prompt,
        max_new_tokens=limit,
        temperature=0.0,
    )

    metrics = compare_summaries(flan_summary, meditron_summary, discharge_text)
    return flan_summary, meditron_summary, metrics


def run_full_multisection(stay_data):
    """
    Run the multi-section final summary:
    dx_proc + labs + meds + measurements + ICU procedures + outputs
    """
    discharge_text = stay_data["discharge_text"]

    # Build features for each component
    dx_features = build_view_dx_proc(stay_data)
    labs_features = build_view_labs(stay_data)
    meds_features = build_view_meds(stay_data)
    meas_features = build_view_measurements(stay_data)
    outputs_features = build_view_outputs(stay_data)
    proc_icu_features = build_view_procedureevents(stay_data)

    # ----------------------
    # FLAN sections
    # ----------------------
    flan_dx = generate_flan(
        make_prompt("dx_proc", dx_features, model_name="flan"),
        max_new_tokens=TOKEN_LIMITS["dx_proc"],
    )
    flan_labs = generate_flan(
        make_prompt("labs", labs_features, model_name="flan"),
        max_new_tokens=TOKEN_LIMITS["labs"],
    )
    flan_meds = generate_flan(
        make_prompt("meds", meds_features, model_name="flan"),
        max_new_tokens=TOKEN_LIMITS["meds"],
    )
    flan_meas = generate_flan(
        make_prompt("measurements", meas_features, model_name="flan"),
        max_new_tokens=TOKEN_LIMITS["measurements"],
    )
    flan_proc_icu = generate_flan(
        make_prompt("procedureevents", proc_icu_features, model_name="flan"),
        max_new_tokens=TOKEN_LIMITS["procedureevents"],
    )
    flan_outputs = generate_flan(
        make_prompt("outputs", outputs_features, model_name="flan"),
        max_new_tokens=TOKEN_LIMITS["outputs"],
    )

    flan_summary = (
        "Diagnosis and admission context:\n"
        + flan_dx.strip()
        + "\n\nLaboratory events during the ICU stay:\n"
        + flan_labs.strip()
        + "\n\nMedications and therapies during the ICU stay:\n"
        + flan_meds.strip()
        + "\n\nICU measurements and clinical course:\n"
        + flan_meas.strip()
        + "\n\nICU bedside procedures and interventions:\n"
        + flan_proc_icu.strip()
        + "\n\nFluid outputs and drains:\n"
        + flan_outputs.strip()
    )

    # ----------------------
    # Meditron sections
    # ----------------------
    # Note: We use temperature=0.0 for deterministic outputs
    med_dx = generate_meditron(
        make_prompt("dx_proc", dx_features, model_name="meditron"),
        max_new_tokens=TOKEN_LIMITS["dx_proc"],
        temperature=0.0,
    )
    med_labs = generate_meditron(
        make_prompt("labs", labs_features, model_name="meditron"),
        max_new_tokens=TOKEN_LIMITS["labs"],
        temperature=0.0,
    )
    med_meds = generate_meditron(
        make_prompt("meds", meds_features, model_name="meditron"),
        max_new_tokens=TOKEN_LIMITS["meds"],
        temperature=0.0,
    )
    med_meas = generate_meditron(
        make_prompt("measurements", meas_features, model_name="meditron"),
        max_new_tokens=TOKEN_LIMITS["measurements"],
        temperature=0.0,
    )
    med_proc_icu = generate_meditron(
        make_prompt("procedureevents", proc_icu_features, model_name="meditron"),
        max_new_tokens=TOKEN_LIMITS["procedureevents"],
        temperature=0.0,
    )
    med_outputs = generate_meditron(
        make_prompt("outputs", outputs_features, model_name="meditron"),
        max_new_tokens=TOKEN_LIMITS["outputs"],
        temperature=0.0,
    )

    meditron_summary = (
        "Diagnosis and admission context:\n"
        + med_dx.strip()
        + "\n\nLaboratory events during the ICU stay:\n"
        + med_labs.strip()
        + "\n\nMedications and therapies during the ICU stay:\n"
        + med_meds.strip()
        + "\n\nICU measurements and clinical course:\n"
        + med_meas.strip()
        + "\n\nICU bedside procedures and interventions:\n"
        + med_proc_icu.strip()
        + "\n\nFluid outputs and drains:\n"
        + med_outputs.strip()
    )

    metrics = compare_summaries(flan_summary, meditron_summary, discharge_text)
    return flan_summary, meditron_summary, metrics


def detect_measure_mentions(text: str, measure_name: str):
    """
    Return (mentioned, has_numeric) for a given measurement name in a text.

    mentioned: True if any synonym appears
    has_numeric: True if there's a digit near the synonym (very rough proxy for numeric value)
    """
    if text is None:
        return False, False

    text = text.lower()
    if not text.strip():
        return False, False

    synonyms = MEASUREMENT_SYNONYMS.get(measure_name, [measure_name.lower()])
    mentioned = False
    has_numeric = False

    for syn in synonyms:
        syn_l = syn.lower()
        idx = text.find(syn_l)
        if idx != -1:
            mentioned = True
            # small window around the synonym
            start = max(0, idx - 20)
            end = min(len(text), idx + len(syn_l) + 20)
            window = text[start:end]
            if re.search(r"\d", window):
                has_numeric = True
            # we can break after first hit
            break

    return mentioned, has_numeric


def build_measure_coverage_df(stay_data, flan_summary: str, med_summary: str) -> pd.DataFrame:
    """
    Build a DataFrame with:
    - measure_name
    - min, median, max, count
    - in_discharge, in_flan, in_meditron (booleans)
    - flan_has_numeric, meditron_has_numeric (booleans)
    """
    from features import build_view_measurements  # local import to avoid cycles

    discharge_text = stay_data["discharge_text"]

    meas_view = build_view_measurements(stay_data)
    meas_list = meas_view.get("measurements_summary", [])

    rows = []
    for row in meas_list:
        name = row.get("measure_name", "")

        in_dis, _ = detect_measure_mentions(discharge_text, name)
        in_flan, flan_has_val = detect_measure_mentions(flan_summary, name)
        in_med, med_has_val = detect_measure_mentions(med_summary, name)

        rows.append({
            "measure_name": name,
            "count": row.get("count", 0),
            "min": row.get("min", None),
            "median": row.get("median", None),
            "max": row.get("max", None),
            "in_discharge": bool(in_dis),
            "in_flan": bool(in_flan),
            "in_meditron": bool(in_med),
            "flan_has_numeric": bool(flan_has_val),
            "meditron_has_numeric": bool(med_has_val),
        })

    if not rows:
        return pd.DataFrame()

    df = pd.DataFrame(rows)
    return df


def main():
    st.title("ICU Stay Summary Explorer")

    cohort = load_cohort()
    # Get all available stay_ids for the dropdown
    stay_ids = sorted(cohort["stay_id"].unique())
    
    # Sidebar controls
    with st.sidebar:
        st.header("Controls")

        # --- FIX: Dropdown instead of Number Input ---
        stay_id = st.selectbox(
            "Select ICU stay_id",
            stay_ids,
            index=0
        )

        view_label = st.selectbox(
            "Choose view to summarise",
            list(VIEW_LABELS.values()),
            index=list(VIEW_LABELS.keys()).index("final"),
        )
        view_type = VIEW_KEYS[view_label]

        show_discharge = st.checkbox("Show actual discharge summary snippet", value=True)
        show_metrics = st.checkbox("Show evaluation metrics", value=True)

        run_button = st.button("Generate summaries")

    # ------------------------------------------------------------------
    # SESSION STATE MANAGEMENT
    # ------------------------------------------------------------------
    if "current_stay_id" not in st.session_state:
        st.session_state["current_stay_id"] = None
    if "current_view_type" not in st.session_state:
        st.session_state["current_view_type"] = None
    if "generated_results" not in st.session_state:
        st.session_state["generated_results"] = None

    if run_button:
        # Load data
        try:
            stay_data = load_all_tables_for_stay(int(stay_id))
        except Exception as e:
            st.error(f"Error loading data for stay_id {stay_id}: {e}")
            return

        with st.spinner("Running models..."):
            if view_type == "final":
                flan_summary, med_summary, metrics = run_full_multisection(stay_data)
            else:
                # Update run_single_view call to use token limits internally if needed
                flan_summary, med_summary, metrics = run_single_view(stay_data, view_type)

        st.session_state["generated_results"] = {
            "flan": flan_summary,
            "meditron": med_summary,
            "metrics": metrics,
            "stay_data": stay_data,
            "discharge_text": stay_data["discharge_text"],
            "subject_id": stay_data["subject_id"],
            "hadm_id": stay_data["hadm_id"]
        }
        st.session_state["current_stay_id"] = stay_id
        st.session_state["current_view_type"] = view_type

    # ------------------------------------------------------------------
    # DISPLAY LOGIC
    # ------------------------------------------------------------------
    if st.session_state["generated_results"] is None:
        st.info("Select a stay_id and click **Generate summaries**.")
        return

    results = st.session_state["generated_results"]
    cached_stay_id = st.session_state["current_stay_id"]
    cached_view_type = st.session_state["current_view_type"]
    stay_data = results["stay_data"]

    # Warning if inputs changed without running
    if cached_stay_id != stay_id or cached_view_type != view_type:
        st.warning(f"⚠ You are viewing results for Stay {cached_stay_id}. Click 'Generate summaries' to update.")

    st.subheader("Selected stay details")
    st.write(f"- **stay_id**: `{cached_stay_id}`")
    st.write(f"- **View**: `{VIEW_LABELS.get(cached_view_type, cached_view_type)}`")

    # --- Vertical Layout ---
    st.markdown("---")
    st.header("1. FLAN-T5 Summary")
    st.success(results["flan"]) 

    st.markdown("---")
    st.header("2. Meditron-7B Summary")
    st.info(results["meditron"])

    # --- Discharge Note ---
    if show_discharge:
        st.markdown("---")
        st.subheader("Reference: Actual Discharge Summary")
        snippet = results["discharge_text"][:2000]
        st.markdown(f"```\n{snippet}\n```")
        if len(results["discharge_text"]) > 2000:
            st.caption("... [truncated] ...")

    # --- Metrics ---
    if show_metrics:
        st.markdown("---")
        st.subheader("Evaluation Metrics")
        fl = results["metrics"]["flan"]
        md = results["metrics"]["meditron"]
        
        metrics_data = {
            "Metric": ["BERTScore Precision", "Embedding similarity", "Avg sentence length"],
            "FLAN-T5": [fl['bert_precision'], fl['embedding_similarity'], fl['avg_sentence_length']],
            "Meditron-7B": [md['bert_precision'], md['embedding_similarity'], md['avg_sentence_length']],
        }
        st.table(pd.DataFrame(metrics_data).set_index("Metric"))

    # ------------------------------------------------------------------
    # VISUALS
    # ------------------------------------------------------------------
    
    # Get ICU Intime for relative plotting
    icu_intime = None
    if not stay_data["icu"]["icustays"].empty:
         icu_intime = pd.to_datetime(stay_data["icu"]["icustays"].iloc[0]["intime"])

    # Tables
    if cached_view_type in ("admission", "dx_proc", "final"):
        st.markdown("---")
        st.subheader("Structured Data Tables")
        if cached_view_type in ("admission", "final"):
            render_admission_table(stay_data)
        if cached_view_type in ("dx_proc", "final"):
            render_diagnoses_table(stay_data)
            render_hosp_procedures_table(stay_data)
        if cached_view_type in ("procedureevents", "final"):
             st.markdown("#### ICU Bedside Procedures")
             render_icu_procedureevents_table(stay_data)

    # Charts
    if cached_view_type in ("meds", "final"):
        st.markdown("---")
        st.subheader("Medications")
        render_medications_visuals(stay_data, icu_intime=icu_intime)

    if cached_view_type in ("measurements", "final"):
        st.markdown("---")
        st.subheader("Measurements / Vitals")
        render_measurements_visuals(stay_data, icu_intime=icu_intime)

    if cached_view_type in ("labs", "final"):
        st.markdown("---")
        st.subheader("Lab Tests")
        render_labs_visuals(stay_data, icu_intime=icu_intime)


if __name__ == "__main__":
    main()

================================================================================
FILE: dump_full_project_snapshot.py
================================================================================
#!/usr/bin/env python3
"""
Generate a text snapshot of the project with 3 sections:

1. Overall file structure of the project — only including the main files / coding files and scripts.
2. All supporting files that help in the project set up etc but not contributing to the final output/project.
3. All the code in the coding files exactly copy pasted in an appropriate manner.

Important: All .py files under mimic_llm/archive/ are treated as supporting only.
Their code is NOT copied into Section 3.
"""

from pathlib import Path


# ---- Paths ----

# Home directory (e.g. /home/soham_shah)
HOME = Path.home()

# Assume this script lives in the project root: /home/soham_shah/mimic_llm
PROJECT_ROOT = Path(__file__).resolve().parent

# Output file inside the project root
OUTPUT_PATH = PROJECT_ROOT / "project_full_snapshot.txt"


# Supporting dirs at $HOME level (environment / setup, not core output)
HOME_SUPPORTING_DIRS = [
    HOME / ".cache",
    HOME / ".conda",
    HOME / ".config",
    HOME / ".dotnet",
    HOME / ".ipython",
    HOME / ".jupyter",
    HOME / ".local",
    HOME / ".mamba",
    HOME / ".nv",
    HOME / ".streamlit",
    HOME / ".vscode-server",
]

# Supporting dirs/files inside the project (setup, logs, archives, etc.)
PROJECT_SUPPORTING_DIRS = [
    PROJECT_ROOT / "archive",
    PROJECT_ROOT / "exports",
    PROJECT_ROOT / "logs",
    PROJECT_ROOT / "notebooks",
    PROJECT_ROOT / "__pycache__",  # may or may not exist
]

PROJECT_SUPPORTING_FILES = [
    PROJECT_ROOT / "project_snapshot.txt",
]


# ---- Helpers ----

def is_under(path: Path, parent: Path) -> bool:
    """Return True if path is somewhere under parent."""
    try:
        path.relative_to(parent)
        return True
    except ValueError:
        return False


def is_main_code_file(path: Path) -> bool:
    """
    Return True for .py files that are part of the main pipeline / final output.

    Rules:
    - Must be under PROJECT_ROOT.
    - Must have .py extension.
    - Must NOT be inside archive/.
    - Must NOT be inside __pycache__.
    """
    if path.suffix != ".py":
        return False

    if not is_under(path, PROJECT_ROOT):
        return False

    # Exclude archive (supporting only)
    if is_under(path, PROJECT_ROOT / "archive"):
        return False

    # Exclude any __pycache__ just in case
    if "__pycache__" in path.parts:
        return False

    return True


def collect_main_code_files():
    """Find all main .py files in the project (excluding archive/)."""
    files = []
    for p in PROJECT_ROOT.rglob("*.py"):
        if is_main_code_file(p):
            files.append(p)
    # Sort by relative path for stable ordering
    return sorted(files, key=lambda p: str(p.relative_to(PROJECT_ROOT)))


def collect_supporting_items():
    """Collect supporting dirs/files (environment + project-level support)."""
    items = []

    # Home-level supporting dirs
    for p in HOME_SUPPORTING_DIRS:
        if p.exists():
            items.append(p)

    # Project-level supporting dirs/files
    for p in PROJECT_SUPPORTING_DIRS + PROJECT_SUPPORTING_FILES:
        if p.exists():
            items.append(p)

    # Also list individual files inside archive/ as supporting
    archive_dir = PROJECT_ROOT / "archive"
    if archive_dir.exists():
        for p in archive_dir.rglob("*"):
            if p.is_file():
                items.append(p)

    # De-duplicate
    seen = set()
    unique_items = []
    for p in items:
        s = str(p)
        if s not in seen:
            unique_items.append(p)
            seen.add(s)

    return sorted(unique_items, key=str)


# ---- Formatting sections ----

def format_section_1(main_files):
    """
    Section 1: Overall file structure of the project — only including
    the main files / coding files and scripts.
    """
    lines = []
    lines.append("SECTION 1: Main project file structure (core .py files and scripts)")
    lines.append("")
    lines.append(f"Project root: {PROJECT_ROOT}")
    lines.append("")

    for p in main_files:
        rel = p.relative_to(PROJECT_ROOT)
        # e.g. "app_streamlit.py" or "scripts/make_cohort_icu_250.py"
        lines.append(str(rel))

    lines.append("")  # trailing blank line
    return "\n".join(lines)


def format_section_2(supporting_items):
    """
    Section 2: All supporting files that help in the project set up etc
    but not contributing directly to the final output/project.
    """
    lines = []
    lines.append("SECTION 2: Supporting / setup files and directories")
    lines.append("")
    lines.append("These paths are environment/config/logging/archive/etc., not part of the final output.")
    lines.append("")

    for p in supporting_items:
        prefix = "DIR " if p.is_dir() else "FILE"
        lines.append(f"{prefix}: {p}")

    lines.append("")
    return "\n".join(lines)


def format_section_3(main_files):
    """
    Section 3: All the code in the coding files exactly copy pasted.
    Only includes main .py files (excludes archive/ by design).
    """
    lines = []
    lines.append("SECTION 3: Full source code of main project files")
    lines.append("")

    for p in main_files:
        rel = p.relative_to(PROJECT_ROOT)
        lines.append("=" * 80)
        lines.append(f"FILE: {rel}")
        lines.append("=" * 80)

        try:
            text = p.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            # Fallback if there are unexpected encodings
            text = p.read_text(errors="replace")

        # Append the file contents exactly, without modification
        lines.append(text)
        lines.append("")  # blank line between files

    return "\n".join(lines)


# ---- Main ----

def main():
    if not PROJECT_ROOT.exists():
        raise SystemExit(f"Project root does not exist: {PROJECT_ROOT}")

    main_files = collect_main_code_files()
    supporting_items = collect_supporting_items()

    section1 = format_section_1(main_files)
    section2 = format_section_2(supporting_items)
    section3 = format_section_3(main_files)

    content = "\n".join([
        section1,
        "\n" + "-" * 80 + "\n",
        section2,
        "\n" + "-" * 80 + "\n",
        section3,
        "",
    ])

    OUTPUT_PATH.write_text(content, encoding="utf-8")
    print(f"Wrote snapshot to: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()

================================================================================
FILE: dump_project_snapshot.py
================================================================================
#!/usr/bin/env python3
import os
from pathlib import Path
from datetime import datetime

# ---------- CONFIG ----------

PROJECT_ROOT = Path("/home/soham_shah/mimic_llm")
OUTPUT_PATH = PROJECT_ROOT / "project_snapshot.txt"

# Coding files you listed
CODING_FILE_PATHS = [
    "/home/soham_shah/mimic_llm/scripts",
    "/home/soham_shah/mimic_llm/scripts/check_cohort_discharge_consistency.py",
    "/home/soham_shah/mimic_llm/scripts/check_stayid_hadmid_consistency.py",
    "/home/soham_shah/mimic_llm/scripts/debug_prompt_single_stay.py",
    "/home/soham_shah/mimic_llm/scripts/explore_mimic_proc_stats.py",
    "/home/soham_shah/mimic_llm/scripts/explore_proc_structure.py",
    "/home/soham_shah/mimic_llm/scripts/export_three_stays_jsonl.py",
    "/home/soham_shah/mimic_llm/scripts/export_three_stays_txt.py",
    "/home/soham_shah/mimic_llm/scripts/filter_diagnoses_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_discharge_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_icustays_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_lab_tests_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_measurements_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_medications_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_outputevents_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_patients_admissions_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_procedureevents_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/filter_procedures_to_cohort.py",
    "/home/soham_shah/mimic_llm/scripts/inspect_single_stay.py",
    "/home/soham_shah/mimic_llm/scripts/make_cohort_icu_250.py",
    "/home/soham_shah/mimic_llm/scripts/make_diagnoses_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_discharge_notes_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_icustays_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_lab_tests_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_measurements_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_medications_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_outputevents_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_patients_admissions_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_procedureevents_clean.py",
    "/home/soham_shah/mimic_llm/scripts/make_procedures_clean.py",
    "/home/soham_shah/mimic_llm/scripts/run_single_stay_inference.py",
    "/home/soham_shah/mimic_llm/app_streamlit.py",
    "/home/soham_shah/mimic_llm/eval.py",
    "/home/soham_shah/mimic_llm/features.py",
    "/home/soham_shah/mimic_llm/models.py",
    "/home/soham_shah/mimic_llm/paths.py",
    "/home/soham_shah/mimic_llm/prompts.py",
    "/home/soham_shah/mimic_llm/visuals.py",
]

# Supporting paths you listed (names only, no contents will be dumped)
SUPPORTING_PATHS = [
    "/home/soham_shah/.cache",
    "/home/soham_shah/.conda",
    "/home/soham_shah/.config",
    "/home/soham_shah/.dotnet",
    "/home/soham_shah/.ipython",
    "/home/soham_shah/.jupyter",
    "/home/soham_shah/.local",
    "/home/soham_shah/.mamba",
    "/home/soham_shah/.nv",
    "/home/soham_shah/.streamlit",
    "/home/soham_shah/.vscode-server",
    "/home/soham_shah/mimic_llm",
    "/home/soham_shah/mimic_llm/__pycache__",
    "/home/soham_shah/mimic_llm/exports",
    "/home/soham_shah/mimic_llm/logs",
    "/home/soham_shah/mimic_llm/notebooks",
]

# How deep the "broad" file structure tree should go
MAX_TREE_DEPTH = 2

# ---------- HELPERS ----------

def tree_lines(root: Path, max_depth: int = 3):
    """
    Return a list of lines representing a simple directory tree
    rooted at `root`, limited to `max_depth` levels.
    """
    lines = []

    def walk(path: Path, depth: int):
        indent = "    " * depth
        # Show directory name
        if depth == 0:
            lines.append(f"{path} /")
        else:
            lines.append(f"{indent}{path.name}/")

        if depth >= max_depth:
            return

        try:
            entries = sorted(path.iterdir(), key=lambda p: (p.is_file(), p.name.lower()))
        except (PermissionError, FileNotFoundError) as e:
            lines.append(f"{indent}  [Error reading directory: {e}]")
            return

        for entry in entries:
            if entry.is_dir():
                walk(entry, depth + 1)
            else:
                lines.append(f"{indent}    {entry.name}")

    if root.exists():
        walk(root, 0)
    else:
        lines.append(f"[MISSING] {root}")

    return lines


def resolve_coding_files():
    """
    Take the provided coding file paths, expand the scripts directory,
    and return a sorted list of unique .py files.
    """
    paths = set()

    for p_str in CODING_FILE_PATHS:
        p = Path(p_str)
        if p.is_dir():
            # include all .py files under this directory (recursively)
            for sub in p.rglob("*.py"):
                if sub.is_file():
                    paths.add(sub.resolve())
        else:
            # single file
            if p.suffix == ".py":
                paths.add(p.resolve())

    return sorted(paths)


def write_header(f, title: str):
    f.write("\n\n")
    f.write("=" * 80 + "\n")
    f.write(title + "\n")
    f.write("=" * 80 + "\n\n")


# ---------- MAIN ----------

def main():
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    with OUTPUT_PATH.open("w", encoding="utf-8") as out:
        # Top-level metadata
        out.write("PROJECT SNAPSHOT\n")
        out.write("-" * 80 + "\n")
        out.write(f"Generated at: {datetime.now().isoformat(timespec='seconds')}\n")
        out.write(f"Project root: {PROJECT_ROOT}\n")
        out.write("-" * 80 + "\n")

        # 1) Broad file structure
        write_header(out, "1. BROAD FILE STRUCTURE (limited depth)")
        out.write(f"Root: {PROJECT_ROOT}\n\n")
        for line in tree_lines(PROJECT_ROOT, max_depth=MAX_TREE_DEPTH):
            out.write(line + "\n")

        # 2) Code contents
        write_header(out, "2. CODE FILE CONTENTS")
        coding_files = resolve_coding_files()
        if not coding_files:
            out.write("[No coding files found based on the provided paths]\n")
        else:
            for path in coding_files:
                out.write(f"\n----- BEGIN FILE: {path} -----\n")
                try:
                    with path.open("r", encoding="utf-8", errors="replace") as f_in:
                        for line in f_in:
                            out.write(line)
                except Exception as e:
                    out.write(f"[Error reading file {path}: {e}]\n")
                out.write(f"\n----- END FILE: {path} -----\n")

        # 3) Supporting paths (names only)
        write_header(out, "3. SUPPORTING FILES AND FOLDERS (names only)")
        out.write("The following supporting paths were provided. Only their paths are listed;\n")
        out.write("no contents are included.\n\n")

        for p_str in SUPPORTING_PATHS:
            p = Path(p_str)
            status = ""
            if not p.exists():
                status = " [MISSING]"
            elif p.is_dir():
                status = " [DIR]"
            elif p.is_file():
                status = " [FILE]"

            out.write(f"{p}{status}\n")

    print(f"Done. Snapshot written to: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()

================================================================================
FILE: eval.py
================================================================================
"""
eval.py

Evaluation utilities for model-generated ICU summaries.

Metrics:
- BERTScore Precision (content faithfulness vs discharge summary)
- Embedding similarity (style + content closeness)
- Average sentence length
- Medical term density (% of tokens that are medical-ish)
"""

from typing import Dict
import re

import torch
from bert_score import score as bert_score
from sentence_transformers import SentenceTransformer


# Lazy-loaded embedding model
_embed_model = None

# Simple list of common ICU / medical tokens to approximate "medical tone"
_MEDICAL_TERMS = {
    "sepsis", "septic", "pneumonia", "respiratory", "failure", "ventilation", "ventilated",
    "intubated", "vasopressor", "norepinephrine", "dopamine", "epinephrine", "shock",
    "hemodynamic", "hemodynamically", "hypotension", "hypertension", "tachycardia",
    "bradycardia", "renal", "kidney", "creatinine", "dialysis", "crrt", "cardiac",
    "myocardial", "infarction", "ischemia", "stroke", "antibiotic", "anticoagulation",
    "insulin", "sedation", "propofol", "midazolam", "fentanyl", "icu", "intensive",
    "unit", "monitoring", "lactate", "acidosis", "alkalosis", "hypoxia", "hypercapnia",
}


def _get_embed_model() -> SentenceTransformer:
    global _embed_model
    if _embed_model is None:
        # Compact, fast general-purpose embedding model on CPU only
        _embed_model = SentenceTransformer(
            "sentence-transformers/all-MiniLM-L6-v2",
            device="cpu",
        )
    return _embed_model


def bert_precision(pred: str, ref: str, lang: str = "en") -> float:
    """
    Compute BERTScore Precision between pred and ref.
    """
    if not pred.strip() or not ref.strip():
        return 0.0

    P, R, F1 = bert_score(
    [pred],
    [ref],
    lang=lang,
    verbose=False,
    device="cpu",   # force CPU for BERTScore
)

    return float(P[0])


def embedding_similarity(pred: str, ref: str) -> float:
    """
    Cosine similarity between sentence embeddings of pred and ref.
    Approximates style + content closeness.
    """
    if not pred.strip() or not ref.strip():
        return 0.0

    model = _get_embed_model()
    embeddings = model.encode([pred, ref], convert_to_tensor=True)
    v1, v2 = embeddings[0], embeddings[1]
    sim = torch.nn.functional.cosine_similarity(v1, v2, dim=0).item()
    return float(sim)


def avg_sentence_length(text: str) -> float:
    """
    Average sentence length in tokens (whitespace-split) based on a simple
    sentence split heuristic.
    """
    if not text.strip():
        return 0.0

    # naive sentence split
    sentences = re.split(r"[.!?]+", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    if not sentences:
        return 0.0

    token_counts = [len(s.split()) for s in sentences if s.split()]
    if not token_counts:
        return 0.0

    return float(sum(token_counts) / len(token_counts))


def medical_term_density(text: str) -> float:
    """
    Approximate "medical-ness": fraction of tokens that appear in a small
    domain-specific vocab.

    Returns a value between 0 and 1.
    """
    if not text.strip():
        return 0.0

    tokens = re.findall(r"[A-Za-z]+", text.lower())
    if not tokens:
        return 0.0

    med_hits = sum(1 for t in tokens if t in _MEDICAL_TERMS)
    return float(med_hits / len(tokens))


def compare_summaries(
    flan_summary: str,
    meditron_summary: str,
    discharge_text: str,
) -> Dict[str, Dict[str, float]]:
    """
    Compute evaluation metrics for FLAN and Meditron summaries against
    the actual discharge summary.

    Returns:
        {
            "flan": {...},
            "meditron": {...},
        }
    """
    if not discharge_text.strip():
        # Can't compute reference-based metrics if we have no discharge note
        return {
            "flan": {
                "bert_precision": 0.0,
                "embedding_similarity": 0.0,
                "avg_sentence_length": avg_sentence_length(flan_summary),
                "medical_term_density": medical_term_density(flan_summary),
            },
            "meditron": {
                "bert_precision": 0.0,
                "embedding_similarity": 0.0,
                "avg_sentence_length": avg_sentence_length(meditron_summary),
                "medical_term_density": medical_term_density(meditron_summary),
            },
        }

    flan_metrics = {
        "bert_precision": bert_precision(flan_summary, discharge_text),
        "embedding_similarity": embedding_similarity(flan_summary, discharge_text),
        "avg_sentence_length": avg_sentence_length(flan_summary),
        "medical_term_density": medical_term_density(flan_summary),
    }

    meditron_metrics = {
        "bert_precision": bert_precision(meditron_summary, discharge_text),
        "embedding_similarity": embedding_similarity(meditron_summary, discharge_text),
        "avg_sentence_length": avg_sentence_length(meditron_summary),
        "medical_term_density": medical_term_density(meditron_summary),
    }

    return {"flan": flan_metrics, "meditron": meditron_metrics}

================================================================================
FILE: features.py
================================================================================
import os
import sys
from typing import Dict, Any, List, Optional

import pandas as pd
import numpy as np

# --- wiring to import paths.py from project root ---

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import (  # type: ignore
    COHORT_META_DIR,
    ICU_PROC_COHORT_DIR,
    HOSP_PROC_COHORT_DIR,
    NOTES_PROC_COHORT_DIR,
)


# -------------------------------------------------------------------
# SMALL HELPER UTILITIES
# -------------------------------------------------------------------


def _first_non_null(series: pd.Series) -> Any:
    """Return the first non-null value in a Series, or None."""
    non_null = series.dropna()
    return non_null.iloc[0] if not non_null.empty else None


def _safe_get_single_row(df: pd.DataFrame, where: str) -> pd.Series:
    """
    Return the first row of df if not empty, else an empty Series.

    `where` is only used for error/debug messages.
    """
    if df is None or len(df) == 0:
        return pd.Series(dtype="object")
    return df.iloc[0]


def _calculate_trend(df: pd.DataFrame, time_col: str, val_col: str) -> str:
    """
    Determines if values are Rising, Falling, or Stable over time.
    Uses simple linear regression slope.
    """
    df = df.dropna(subset=[time_col, val_col]).sort_values(time_col)
    if len(df) < 3:
        return "Insufficient data"

    # Convert time to numeric (seconds/hours) for regression
    # (timestamp - min_timestamp) / 1 hour
    x = (df[time_col] - df[time_col].min()).dt.total_seconds() / 3600.0
    y = df[val_col].values

    # Fit line: y = mx + c
    # slope (m) represents change per hour
    try:
        slope, intercept = np.polyfit(x, y, 1)
    except:
        return "Stable"

    # Calculate relative change over the whole period
    total_time_hours = x.max()
    if total_time_hours == 0:
        return "Stable"
    
    total_change = slope * total_time_hours
    start_val = (slope * x.min()) + intercept
    
    # Avoid division by zero
    if start_val == 0: 
        baseline = np.mean(y)
    else:
        baseline = start_val

    pct_change = (total_change / (baseline + 1e-9)) * 100

    # Define thresholds (e.g., >10% change is significant)
    if pct_change > 10:
        return "Rising"
    elif pct_change < -10:
        return "Falling"
    else:
        return "Stable"

# -------------------------------------------------------------------
# CORE LOADER
# -------------------------------------------------------------------

def load_all_tables_for_stay(stay_id: int) -> Dict[str, Any]:
    """
    Load all relevant cohort-filtered tables for a single stay_id.

    Returns a dictionary with:
    - 'stay_id', 'hadm_id', 'subject_id'
    - 'cohort_row' : the row from cohort_icu_250
    - 'icu'  : dict of ICU tables for that stay
    - 'hosp' : dict of hosp tables for that stay/hadm
    - 'discharge_text' : full discharge summary for that hadm_id
    """

    # 1. Load cohort and find the row for this stay_id
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    cohort = pd.read_parquet(cohort_path)

    if stay_id not in set(cohort["stay_id"].unique()):
        raise ValueError(f"stay_id {stay_id} not found in cohort_icu_250")

    cohort_row = cohort[cohort["stay_id"] == stay_id].iloc[0]
    hadm_id = int(cohort_row["hadm_id"])
    subject_id = int(cohort_row["subject_id"])

    # 2. Load ICU tables (already cohort-filtered) and slice by stay_id
    def load_icu_table(name: str) -> pd.DataFrame:
        path = os.path.join(ICU_PROC_COHORT_DIR, name)
        return pd.read_parquet(path)

    icustays = load_icu_table("icustays_clean_icu_250.parquet")
    measurements = load_icu_table("measurements_clean_icu_250.parquet")
    medications = load_icu_table("medications_clean_icu_250.parquet")
    outputevents = load_icu_table("outputevents_clean_icu_250.parquet")
    proc_icu = load_icu_table("procedureevents_clean_icu_250.parquet")

    icustays_this = icustays[icustays["stay_id"] == stay_id].copy()
    measurements_this = measurements[measurements["stay_id"] == stay_id].copy()
    medications_this = medications[medications["stay_id"] == stay_id].copy()
    outputevents_this = outputevents[outputevents["stay_id"] == stay_id].copy()
    proc_icu_this = proc_icu[proc_icu["stay_id"] == stay_id].copy()

    icu_tables = {
        "icustays": icustays_this,
        "measurements": measurements_this,
        "medications": medications_this,
        "outputevents": outputevents_this,
        "procedureevents": proc_icu_this,
    }

    # 3. Load hosp tables (cohort-filtered) and slice by hadm_id / stay_id
    def load_hosp_table(name: str) -> pd.DataFrame:
        path = os.path.join(HOSP_PROC_COHORT_DIR, name)
        return pd.read_parquet(path)

    patadm = load_hosp_table("patients_admissions_clean_icu_250.parquet")
    diagnoses = load_hosp_table("diagnoses_clean_icu_250.parquet")
    procedures = load_hosp_table("procedures_clean_icu_250.parquet")
    labs = load_hosp_table("lab_tests_clean_icu_250.parquet")

    patadm_this = patadm[patadm["hadm_id"] == hadm_id].copy()
    dx_this = diagnoses[diagnoses["hadm_id"] == hadm_id].copy()

    # Procedures & labs ICU-window tables include stay_id; if not, fall back to hadm_id
    if "stay_id" in procedures.columns:
        procs_this = procedures[procedures["stay_id"] == stay_id].copy()
    else:
        procs_this = procedures[procedures["hadm_id"] == hadm_id].copy()

    if "stay_id" in labs.columns:
        labs_this = labs[labs["stay_id"] == stay_id].copy()
    else:
        labs_this = labs[labs["hadm_id"] == hadm_id].copy()

    hosp_tables = {
        "patients_admissions": patadm_this,
        "diagnoses": dx_this,
        "procedures": procs_this,
        "labs": labs_this,
    }

    # 4. Load discharge summary (cohort-filtered)
    discharge_path = os.path.join(
        NOTES_PROC_COHORT_DIR, "discharge_clean_icu_250.parquet"
    )
    discharge = pd.read_parquet(discharge_path)
    disc_this = discharge[discharge["hadm_id"] == hadm_id].copy()

    if len(disc_this) == 0:
        discharge_text = ""
    else:
        # We enforce exactly 1 discharge note per hadm_id in cohort building,
        # but still just take the first row defensively.
        discharge_text = str(disc_this.iloc[0].get("text", ""))

    return {
        "stay_id": stay_id,
        "hadm_id": hadm_id,
        "subject_id": subject_id,
        "cohort_row": cohort_row,
        "icu": icu_tables,
        "hosp": hosp_tables,
        "discharge_text": discharge_text,
    }


# -------------------------------------------------------------------
# VIEW: DEMOGRAPHICS / ADMISSION
# -------------------------------------------------------------------


def build_view_demographics(stay_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build a 'demographics/admission' view.

    Focuses on:
    - anchor_age
    - gender
    - admission_type
    - admission_location
    - discharge_location
    - admittime
    - dischtime
    - hospital_expire_flag
    - deathdate
    """

    patadm = stay_data["hosp"]["patients_admissions"].copy()
    row = _safe_get_single_row(patadm, "patients_admissions")

    def get_safe(col: str, default=None):
        return row[col] if col in row.index else default

    demographics = {
        "anchor_age": get_safe("anchor_age"),
        "gender": get_safe("gender"),
        "admission_type": get_safe("admission_type"),
        "admission_location": get_safe("admission_location"),
        "discharge_location": get_safe("discharge_location"),
        "admittime": get_safe("admittime"),
        "dischtime": get_safe("dischtime"),
        "hospital_expire_flag": get_safe("hospital_expire_flag"),
        "deathdate": get_safe("deathdate"),
    }

    return {"demographics": demographics}


# -------------------------------------------------------------------
# VIEW: DIAGNOSES (HOSPITAL)
# -------------------------------------------------------------------


def build_view_diagnoses(stay_data: Dict[str, Any], max_diagnoses: int = 15) -> Dict[str, Any]:
    """
    Build the 'diagnoses-only' view for a stay.

    Uses:
    - diagnoses_clean_icu_250 (HOSP table, filtered by hadm_id)

    We only care about:
    - dx_long_title (human-readable diagnosis)
    - dx_seq_num (for ordering)

    Returns:
        {
            "diagnoses": [
                {"sequence": 1, "title": "..."},
                ...
            ]
        }
    """

    dx = stay_data["hosp"]["diagnoses"].copy()

    if "dx_long_title" not in dx.columns:
        return {"diagnoses": []}

    # Sort by sequence number if available
    if "dx_seq_num" in dx.columns:
        dx_sorted = dx.sort_values(["hadm_id", "dx_seq_num"])
    else:
        dx_sorted = dx.copy()

    diagnoses_list: List[Dict[str, Any]] = []
    for _, r in dx_sorted.iterrows():
        diagnoses_list.append(
            {
                "sequence": r.get("dx_seq_num"),
                "title": r.get("dx_long_title"),
            }
        )

    diagnoses_list = diagnoses_list[:max_diagnoses]
    return {"diagnoses": diagnoses_list}


# -------------------------------------------------------------------
# VIEW: HOSPITAL PROCEDURES (ICU-window)
# -------------------------------------------------------------------


def build_view_hosp_procedures(
    stay_data: Dict[str, Any], max_procs: Optional[int] = None
) -> Dict[str, Any]:
    """
    Build the 'HOSP procedures during ICU window' view for a stay.

    Uses:
    - procedures_clean_icu_250, filtered by stay_id (ICU-window pre-filtered)

    We care about:
    - proc_seq_num (for ordering if present)
    - proc_long_title (human-readable procedure)
    - procedure_chartdatetime (approximate time, if present)
    """

    procs = stay_data["hosp"]["procedures"].copy()
    if procs.empty:
        return {"procedures_hosp": []}

    # Identify key columns
    title_col = "proc_long_title" if "proc_long_title" in procs.columns else None
    if title_col is None:
        # fall back to any 'long_title' or 'label' column
        for c in procs.columns:
            if "long_title" in c or "label" in c:
                title_col = c
                break

    time_col = None
    for c in procs.columns:
        if "charttime" in c or "datetime" in c or c.endswith("_date") or c.endswith("_time"):
            time_col = c
            break

    seq_col = "proc_seq_num" if "proc_seq_num" in procs.columns else None

    # Sort by sequence or time
    if seq_col is not None:
        procs = procs.sort_values([seq_col])
    elif time_col is not None:
        procs = procs.sort_values([time_col])

    procedures_list: List[Dict[str, Any]] = []
    for _, r in procs.iterrows():
        procedures_list.append(
            {
                "sequence": r.get(seq_col) if seq_col is not None else None,
                "title": r.get(title_col) if title_col is not None else None,
                "time": r.get(time_col) if time_col is not None else None,
            }
        )

    if max_procs is not None:
        procedures_list = procedures_list[:max_procs]

    return {"procedures_hosp": procedures_list}


# -------------------------------------------------------------------
# VIEW: ICU PROCEDUREEVENTS (bedside procedures)
# -------------------------------------------------------------------


def build_view_icu_procedures(
    stay_data: Dict[str, Any], max_events: Optional[int] = None
) -> Dict[str, Any]:
    """
    Build the ICU 'procedureevents' view for a stay.

    Uses:
    - procedureevents_clean_icu_250 (ICU table, filtered by stay_id)

    We focus on:
    - procedureevents_label
    - procedureevents_category
    - procedureevents_location
    - procedureevents_value, procedureevents_valueuom
    - procedureevents_start_date, procedureevents_end_date

    Returns:
        {
            "procedureevents": [
                {
                    "label": ...,
                    "category": ...,
                    "location": ...,
                    "value": ...,
                    "valueuom": ...,
                    "start": ...,
                    "end": ...,
                },
                ...
            ]
        }
    """

    proc_icu = stay_data["icu"]["procedureevents"].copy()
    if proc_icu.empty:
        return {"procedureevents": []}

    label_col = "procedureevents_label" if "procedureevents_label" in proc_icu.columns else None
    cat_col = "procedureevents_category" if "procedureevents_category" in proc_icu.columns else None
    loc_col = "procedureevents_location" if "procedureevents_location" in proc_icu.columns else None
    val_col = "procedureevents_value" if "procedureevents_value" in proc_icu.columns else None
    val_uom_col = (
        "procedureevents_valueuom" if "procedureevents_valueuom" in proc_icu.columns else None
    )

    start_col = None
    for c in proc_icu.columns:
        if "start" in c and ("date" in c or "time" in c):
            start_col = c
            break

    end_col = None
    for c in proc_icu.columns:
        if "end" in c and ("date" in c or "time" in c):
            end_col = c
            break

    # Sort by start time if available
    if start_col is not None:
        proc_icu = proc_icu.sort_values([start_col])

    events: List[Dict[str, Any]] = []
    for _, r in proc_icu.iterrows():
        events.append(
            {
                "label": r.get(label_col) if label_col is not None else None,
                "category": r.get(cat_col) if cat_col is not None else None,
                "location": r.get(loc_col) if loc_col is not None else None,
                "value": r.get(val_col) if val_col is not None else None,
                "valueuom": r.get(val_uom_col) if val_uom_col is not None else None,
                "start": r.get(start_col) if start_col is not None else None,
                "end": r.get(end_col) if end_col is not None else None,
            }
        )

    if max_events is not None:
        events = events[:max_events]

    return {"procedureevents_summary": events}  


# -------------------------------------------------------------------
# VIEW: Procedure events
# -------------------------------------------------------------------

def build_view_procedureevents(
    stay_data: Dict[str, Any], max_events: Optional[int] = None
) -> Dict[str, Any]:
    """
    Thin alias for ICU procedureevents, for clarity.

    This makes it clear that these are the ICU `procedureevents` table
    (not HOSP procedures). It simply forwards to `build_view_icu_procedures`.
    """
    return build_view_icu_procedures(stay_data, max_events=max_events)


# -------------------------------------------------------------------
# VIEW: LABS (ICU-window)
# -------------------------------------------------------------------

def build_view_labs(stay_data: Dict[str, Any], max_labs: int = 10) -> Dict[str, Any]:
    """
    Build the 'labs-only' view for a stay.
    Now includes Trend calculation and cleaner Unit extraction.
    """
    labs = stay_data["hosp"]["labs"].copy()
    if labs.empty:
        return {"labs_summary": []}

    # Identify key columns
    label_col = None
    for c in labs.columns:
        if c == "lab_tests_label" or (c.endswith("label") and "lab_tests_" in c):
            label_col = c
            break

    val_col = None
    for c in labs.columns:
        if "valuenum" in c:
            val_col = c
            break

    # Identify unit column
    unit_col = None
    for c in labs.columns:
        if "valueuom" in c:
            unit_col = c
            break

    warning_col = None
    for c in labs.columns:
        if "warning" in c:
            warning_col = c
            break

    fluid_col = "lab_tests_fluid" if "lab_tests_fluid" in labs.columns else None
    cat_col = "lab_tests_category" if "lab_tests_category" in labs.columns else None

    # Identify time column for trends
    time_col = None
    for c in labs.columns:
        if "charttime" in c or "datetime" in c or c.endswith("_time") or c.endswith("_date"):
            time_col = c
            break

    if label_col is None or val_col is None:
        return {"labs_summary": []}

    labs[val_col] = pd.to_numeric(labs[val_col], errors="coerce")

    if warning_col is not None:
        labs[warning_col] = pd.to_numeric(labs[warning_col], errors="coerce").fillna(0)

    grouped = labs.groupby(label_col)
    labs_summary: List[Dict[str, Any]] = []

    for label, group in grouped:
        vals = group[val_col].dropna()
        if len(vals) == 0:
            continue

        count = len(group)
        min_val = vals.min()
        max_val = vals.max()
        median_val = vals.median()

        if warning_col is not None:
            abnormal_count = int((group[warning_col] == 1).sum())
        else:
            abnormal_count = 0

        # 1. Extract Unit (Mode)
        unit = None
        if unit_col is not None and unit_col in group.columns:
            uoms = group[unit_col].dropna()
            if not uoms.empty:
                unit = uoms.mode().iloc[0]

        # 2. Calculate Trend
        trend = "Unknown"
        if time_col is not None and time_col in group.columns:
            trend = _calculate_trend(group, time_col, val_col)

        fluid = (
            _first_non_null(group[fluid_col]) if fluid_col is not None and fluid_col in group.columns else None
        )
        category = (
            _first_non_null(group[cat_col]) if cat_col is not None and cat_col in group.columns else None
        )

        labs_summary.append(
            {
                "lab_name": str(label),
                "fluid": None if pd.isna(fluid) else fluid,
                "category": None if pd.isna(category) else category,
                "count": int(count),
                "abnormal_count": abnormal_count,
                "min": float(min_val),
                "max": float(max_val),
                "median": float(median_val),
                "unit": unit,   # <--- Added
                "trend": trend, # <--- Added
            }
        )

    # Sort by abnormal_count (desc), then by count
    labs_summary = sorted(
        labs_summary,
        key=lambda x: (x["abnormal_count"], x["count"]),
        reverse=True,
    )

    return {"labs_summary": labs_summary[:max_labs]}


# -------------------------------------------------------------------
# VIEW: MEDICATIONS (ICU meds)
# -------------------------------------------------------------------

def build_view_meds(stay_data: Dict[str, Any], max_meds: int = 10) -> Dict[str, Any]:
    """
    Build the 'meds-only' view for a stay.
    UPDATED: Performs 'Safe Summation' by only summing amounts that match the dominant unit.
    """
    meds = stay_data["icu"]["medications"].copy()
    if meds.empty:
        return {"meds_summary": []}

    # Identify label col
    label_col = None
    for c in meds.columns:
        if c == "medications_label" or (c.endswith("label") and "medications_" in c):
            label_col = c
            break
    if label_col is None:
        for c in meds.columns:
            if "label" in c:
                label_col = c
                break

    if label_col is None:
        return {"meds_summary": []}

    cat_col = "medications_category" if "medications_category" in meds.columns else None

    # Identify amount & unit columns
    amount_col = None
    for c in meds.columns:
        if "amount" in c and "original" not in c and "uom" not in c:
            amount_col = c
            break
    
    amount_uom_col = None
    for c in meds.columns:
        if "amountuom" in c and "original" not in c:
            amount_uom_col = c
            break

    # Identify time columns
    start_col = None
    for c in meds.columns:
        if "start" in c and ("date" in c or "time" in c):
            start_col = c
            break
    end_col = None
    for c in meds.columns:
        if "end" in c and ("date" in c or "time" in c):
            end_col = c
            break

    grouped = meds.groupby(label_col)
    meds_summary: List[Dict[str, Any]] = []

    for label, group in grouped:
        count = len(group)

        # Category (take most frequent)
        category = None
        if cat_col is not None and cat_col in group.columns:
            non_null = group[cat_col].dropna()
            category = non_null.mode().iloc[0] if not non_null.empty else None

        # --- SAFE SUMMATION LOGIC ---
        total_amount = None
        unit = None

        # 1. Determine the Dominant Unit (Mode)
        if amount_uom_col is not None and amount_uom_col in group.columns:
            uoms = group[amount_uom_col].dropna()
            if not uoms.empty:
                unit = uoms.mode().iloc[0]
        
        # 2. Sum ONLY rows that match the dominant unit
        if amount_col is not None and amount_col in group.columns:
            # If we found a unit, filter by it
            if unit is not None and amount_uom_col in group.columns:
                mask = group[amount_uom_col] == unit
                vals = pd.to_numeric(group.loc[mask, amount_col], errors="coerce").dropna()
            else:
                # Fallback: if no unit info exists, sum everything (legacy behavior)
                vals = pd.to_numeric(group[amount_col], errors="coerce").dropna()
            
            if not vals.empty:
                total_amount = float(vals.sum())

        # Start/end times
        start_min = None
        end_max = None
        if start_col is not None and start_col in group.columns:
            start_times = pd.to_datetime(group[start_col], errors="coerce")
            if not start_times.dropna().empty:
                start_min = start_times.min()
        if end_col is not None and end_col in group.columns:
            end_times = pd.to_datetime(group[end_col], errors="coerce")
            if not end_times.dropna().empty:
                end_max = end_times.max()

        meds_summary.append(
            {
                "med_name": str(label),
                "category": None if pd.isna(category) else category,
                "num_orders": int(count),
                "total_amount": total_amount,
                "unit": unit,
                "first_start": start_min,
                "last_end": end_max,
            }
        )

    # Sort: more frequently used meds first
    meds_summary = sorted(
        meds_summary,
        key=lambda x: x["num_orders"],
        reverse=True,
    )

    return {"meds_summary": meds_summary[:max_meds]}


"""
def build_view_meds(stay_data: Dict[str, Any], max_meds: int = 25) -> Dict[str, Any]:
    
    Build the 'meds-only' view for a stay.

    Uses:
    - medications_clean_icu_250 (ICU meds table, filtered by stay_id)

    Aggregates by medication label/category:
    - num_orders
    - total_amount
    - first_start
    - last_end
    

    meds = stay_data["icu"]["medications"].copy()
    if meds.empty:
        return {"meds_summary": []}

    # Identify label/category/time/amount cols
    label_col = None
    for c in meds.columns:
        if c == "medications_label" or (c.endswith("label") and "medications_" in c):
            label_col = c
            break
    if label_col is None:
        for c in meds.columns:
            if "label" in c:
                label_col = c
                break

    cat_col = "medications_category" if "medications_category" in meds.columns else None

    start_col = None
    for c in meds.columns:
        if "start" in c and ("date" in c or "time" in c):
            start_col = c
            break

    end_col = None
    for c in meds.columns:
        if "end" in c and ("date" in c or "time" in c):
            end_col = c
            break

    amount_col = None
    for c in meds.columns:
        if "amount" in c and "original" not in c:
            amount_col = c
            break

    if label_col is None:
        return {"meds_summary": []}

    grouped = meds.groupby(label_col)

    meds_summary: List[Dict[str, Any]] = []

    for label, group in grouped:
        count = len(group)

        # Category (take most frequent)
        category = None
        if cat_col is not None and cat_col in group.columns:
            non_null = group[cat_col].dropna()
            category = non_null.mode().iloc[0] if not non_null.empty else None

        # Amount summary if numeric
        total_amount = None
        if amount_col is not None and amount_col in group.columns:
            vals = pd.to_numeric(group[amount_col], errors="coerce").dropna()
            if not vals.empty:
                total_amount = float(vals.sum())

        # Start/end times approximate range
        start_min = None
        end_max = None
        if start_col is not None and start_col in group.columns:
            start_times = pd.to_datetime(group[start_col], errors="coerce")
            if not start_times.dropna().empty:
                start_min = start_times.min()
        if end_col is not None and end_col in group.columns:
            end_times = pd.to_datetime(group[end_col], errors="coerce")
            if not end_times.dropna().empty:
                end_max = end_times.max()

        meds_summary.append(
            {
                "med_name": str(label),
                "category": None if pd.isna(category) else category,
                "num_orders": int(count),
                "total_amount": total_amount,
                "first_start": start_min,
                "last_end": end_max,
            }
        )

    # Sort: more frequently used meds first
    meds_summary = sorted(
        meds_summary,
        key=lambda x: x["num_orders"],
        reverse=True,
    )

    # Limit length
    meds_summary = meds_summary[:max_meds]

    return {"meds_summary": meds_summary}
"""

# -------------------------------------------------------------------
# VIEW: MEASUREMENTS (ICU measurements / chartevents)
# -------------------------------------------------------------------

def build_view_measurements(
    stay_data: Dict[str, Any], max_labels: int = 10
) -> Dict[str, Any]:
    """
    Build the 'measurements-only' view for a stay.
    Now includes Unit extraction and Trend calculation.
    """
    meas = stay_data["icu"]["measurements"].copy()
    if meas.empty:
        return {"measurements_summary": []}

    # Identify label column
    label_col = None
    for c in meas.columns:
        if c == "measurements_label" or (c.endswith("label") and "measurements_" in c):
            label_col = c
            break
    if label_col is None:
        for c in meas.columns:
            if "label" in c:
                label_col = c
                break

    # Identify value column
    val_col = None
    for c in meas.columns:
        if "valuenum" in c:
            val_col = c
            break
    
    # Identify unit column
    val_uom_col = None
    for c in meas.columns:
        if "valueuom" in c:
            val_uom_col = c
            break

    # Identify time column (critical for trends)
    time_col = None
    for c in meas.columns:
        if "charttime" in c or "datetime" in c or c.endswith("_time") or c.endswith("_date"):
            time_col = c
            break

    if label_col is None or val_col is None:
        return {"measurements_summary": []}

    meas[val_col] = pd.to_numeric(meas[val_col], errors="coerce")

    # Optionally focus on the most frequent measurement labels for this stay
    label_counts = meas[label_col].value_counts()
    top_labels = label_counts.head(max_labels).index.tolist()
    meas = meas[meas[label_col].isin(top_labels)]

    grouped = meas.groupby(label_col)
    measurements_summary: List[Dict[str, Any]] = []

    for label, group in grouped:
        vals = group[val_col].dropna()
        if len(vals) == 0:
            continue

        count = len(group)
        min_val = vals.min()
        max_val = vals.max()
        median_val = vals.median()
        
        # 1. Extract Unit (Mode)
        unit = None
        if val_uom_col is not None and val_uom_col in group.columns:
            uoms = group[val_uom_col].dropna()
            if not uoms.empty:
                unit = uoms.mode().iloc[0]

        # 2. Calculate Trend
        trend = "Unknown"
        if time_col is not None and time_col in group.columns:
            # Uses the helper function you added: _calculate_trend
            trend = _calculate_trend(group, time_col, val_col)

        measurements_summary.append(
            {
                "measure_name": str(label),
                "count": int(count),
                "min": float(min_val),
                "max": float(max_val),
                "median": float(median_val),
                "unit": unit,   # <--- Added
                "trend": trend, # <--- Added
            }
        )

    # Sort by count
    measurements_summary = sorted(
        measurements_summary,
        key=lambda x: x["count"],
        reverse=True,
    )

    return {"measurements_summary": measurements_summary}


"""
def build_view_measurements(
    stay_data: Dict[str, Any], max_labels: int = 20
) -> Dict[str, Any]:
    
    Build the 'measurements-only' view for a stay.

    Uses:
    - measurements_clean_icu_250 (chartevents-like ICU measurements)

    Aggregates by measurement label:
    - count
    - min / max / median of valuenum
    - optional trend (increasing / decreasing / stable)
    

    meas = stay_data["icu"]["measurements"].copy()
    if meas.empty:
        return {"measurements_summary": []}

    # Identify label & value & time columns
    label_col = None
    for c in meas.columns:
        if c == "measurements_label" or (c.endswith("label") and "measurements_" in c):
            label_col = c
            break
    if label_col is None:
        for c in meas.columns:
            if "label" in c:
                label_col = c
                break

    val_col = None
    for c in meas.columns:
        if "valuenum" in c:
            val_col = c
            break

    time_col = None
    for c in meas.columns:
        if "charttime" in c or "datetime" in c or c.endswith("_time") or c.endswith("_date"):
            time_col = c
            break

    if label_col is None or val_col is None:
        return {"measurements_summary": []}

    meas[val_col] = pd.to_numeric(meas[val_col], errors="coerce")

    # Optionally focus on the most frequent measurement labels for this stay
    label_counts = meas[label_col].value_counts()
    top_labels = label_counts.head(max_labels).index.tolist()

    meas = meas[meas[label_col].isin(top_labels)]

    grouped = meas.groupby(label_col)

    measurements_summary: List[Dict[str, Any]] = []

    for label, group in grouped:
        vals = group[val_col].dropna()
        if len(vals) == 0:
            continue

        count = len(group)
        min_val = vals.min()
        max_val = vals.max()
        median_val = vals.median()
        
         
        # Trend, if time info available
        if time_col is not None:
            trend = _infer_trend_from_times(vals, group[time_col])
        else:
            trend = None
        

        measurements_summary.append(
            {
                "measure_name": str(label),
                "count": int(count),
                "min": float(min_val),
                "max": float(max_val),
                "median": float(median_val),
                # "trend": trend,
            }
        )

    # Sort by count
    measurements_summary = sorted(
        measurements_summary,
        key=lambda x: x["count"],
        reverse=True,
    )

    return {"measurements_summary": measurements_summary}
"""

# -------------------------------------------------------------------
# VIEW: OUTPUTEVENTS (ICU outputs)
# -------------------------------------------------------------------

def build_view_outputs(
    stay_data: Dict[str, Any], max_labels: int = 15
) -> Dict[str, Any]:
    """
    Build the 'outputs-only' view for a stay.

    Uses:
    - outputevents_clean_icu_250 (ICU outputevents table)

    Aggregates by output label:
    - num_records
    - total_value (where numeric)
    - first_time / last_time
    - optional trend
    """

    out = stay_data["icu"]["outputevents"].copy()
    if out.empty:
        return {"outputs_summary": []}

    label_col = "outputevents_label" if "outputevents_label" in out.columns else None
    if label_col is None:
        for c in out.columns:
            if "label" in c:
                label_col = c
                break

    cat_col = "outputevents_category" if "outputevents_category" in out.columns else None
    val_col = "outputevents_value" if "outputevents_value" in out.columns else None
    val_uom_col = (
        "outputevents_valueuom" if "outputevents_valueuom" in out.columns else None
    )

    time_col = None
    for c in out.columns:
        if "charttime" in c or "datetime" in c or c.endswith("_time") or c.endswith("_date"):
            time_col = c
            break

    if label_col is None or val_col is None:
        return {"outputs_summary": []}

    out[val_col] = pd.to_numeric(out[val_col], errors="coerce")

    grouped = out.groupby(label_col)

    outputs_summary: List[Dict[str, Any]] = []

    for label, group in grouped:
        vals = group[val_col].dropna()
        count = len(group)
        total_value = float(vals.sum()) if not vals.empty else None

       

        category = (
            _first_non_null(group[cat_col])
            if cat_col is not None and cat_col in group.columns
            else None
        )
        valueuom = (
            _first_non_null(group[val_uom_col])
            if val_uom_col is not None and val_uom_col in group.columns
            else None
        )

        outputs_summary.append(
            {
                "label": str(label),
                "category": None if pd.isna(category) else category,
                "num_records": int(count),
                "total_value": total_value,
                "valueuom": None if pd.isna(valueuom) else valueuom,
            }
        )

    # Sort by num_records desc
    outputs_summary = sorted(
        outputs_summary,
        key=lambda x: x["num_records"],
        reverse=True,
    )

    outputs_summary = outputs_summary[:max_labels]

    return {"outputs_summary": outputs_summary}


# -------------------------------------------------------------------
# BACKWARDS-COMPATIBLE VIEWS: DX_PROC + ADMISSION
# -------------------------------------------------------------------


def build_view_dx_proc(stay_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build the combined 'diagnoses + HOSP ICU-window procedures + demographics' view.

    This is mainly for backwards compatibility with older prompt code that
    expects a single dict with keys:
        - 'demographics'
        - 'diagnoses'
        - 'procedures'

    Internally, it delegates to:
        - build_view_demographics
        - build_view_diagnoses
        - build_view_hosp_procedures

    and then exposes:
        - 'procedures' (alias of 'procedures_hosp') for compatibility
        - 'procedures_hosp' (explicit)
    """

    demo_view = build_view_demographics(stay_data)
    dx_view = build_view_diagnoses(stay_data)
    procs_view = build_view_hosp_procedures(stay_data)

    demographics = demo_view.get("demographics", {})
    diagnoses = dx_view.get("diagnoses", [])
    procedures_hosp = procs_view.get("procedures_hosp", [])

    return {
        "demographics": demographics,
        "diagnoses": diagnoses,
        # backwards-compatible key:
        "procedures": procedures_hosp,
        # more explicit key:
        "procedures_hosp": procedures_hosp,
    }


def build_view_admission(stay_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build an 'admission/demographics + diagnoses' view.

    This reuses:
        - build_view_demographics
        - build_view_diagnoses
    """
    demo_view = build_view_demographics(stay_data)
    dx_view = build_view_diagnoses(stay_data)
    return {
        "demographics": demo_view.get("demographics", {}),
        "diagnoses": dx_view.get("diagnoses", []),
    }

================================================================================
FILE: models.py
================================================================================
"""
models.py

Model loading and text generation utilities for:
- FLAN-T5 (Seq2Seq, instruction-tuned)
- Meditron-7B (causal LM, medical domain)

Public API:
- load_flan() -> (model, tokenizer)
- load_meditron() -> (model, tokenizer)
- generate_flan(prompt, max_new_tokens=..., num_beams=...)
- generate_meditron(prompt, max_new_tokens=..., temperature=...)
"""

import os
from typing import Tuple

import torch
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    AutoTokenizer,
)

# --------------------------------------------------------------------
# Model names (configurable via env vars)
# --------------------------------------------------------------------

FLAN_MODEL_NAME = os.environ.get("FLAN_MODEL_NAME", "google/flan-t5-large")
MEDITRON_MODEL_NAME = os.environ.get("MEDITRON_MODEL_NAME", "epfl-llm/meditron-7b")

_flan_model = None
_flan_tokenizer = None
_meditron_model = None
_meditron_tokenizer = None


# --------------------------------------------------------------------
# Device and dtype helpers
# --------------------------------------------------------------------

def _use_half() -> bool:
    """Return True if we should use float16 (on GPU)."""
    return torch.cuda.is_available()


def _dtype():
    """Return the torch dtype for model weights."""
    return torch.float16 if _use_half() else torch.float32


def _pick_device(min_free_gb: float = 8.0) -> torch.device:
    """
    Pick a device for inference.

    For simplicity and robustness against meta-tensor issues:
    - Use cuda:0 if CUDA is available.
    - Otherwise fall back to CPU.

    (The original version scanned GPU memory; here we keep it
    simpler and more predictable.)
    """
    if not torch.cuda.is_available():
        print("[models] CUDA not available, using CPU.")
        return torch.device("cpu")

    device = torch.device("cuda:0")
    props = torch.cuda.get_device_properties(device)
    total_gb = props.total_memory / (1024 ** 3)
    print(f"[models] Using {device} for inference (total VRAM ~{total_gb:.1f} GB).")

    if total_gb < min_free_gb:
        print(
            f"[models] GPU has < {min_free_gb} GB total memory; "
            "you may see OOM if other jobs are running."
        )
    return device


# --------------------------------------------------------------------
# FLAN-T5 loading
# --------------------------------------------------------------------

def load_flan() -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
    """
    Lazy-load FLAN-T5 on a suitable device.

    Key difference vs the old version: we avoid calling .to() on
    models that might live on 'meta' by loading directly with the
    correct device / device_map.
    """
    global _flan_model, _flan_tokenizer

    if _flan_model is not None and _flan_tokenizer is not None:
        return _flan_model, _flan_tokenizer

    device = _pick_device(min_free_gb=8.0)
    print(f"[models] Loading FLAN-T5 from {FLAN_MODEL_NAME} on {device}...")

    _flan_tokenizer = AutoTokenizer.from_pretrained(FLAN_MODEL_NAME)

    # Load directly with an appropriate device configuration.
    if device.type == "cuda":
        # Use device_map="auto" so HF/accelerate handles placement.
        # No manual .to(device) to avoid meta-tensor errors.
        _flan_model = AutoModelForSeq2SeqLM.from_pretrained(
            FLAN_MODEL_NAME,
            torch_dtype=_dtype(),
            device_map="auto",
        )
    else:
        # CPU: simple load, then .to(device) (CPU) is safe.
        _flan_model = AutoModelForSeq2SeqLM.from_pretrained(
            FLAN_MODEL_NAME,
            torch_dtype=_dtype(),
        )
        _flan_model.to(device)

    # If, for some reason, the model still has 'meta' parameters, reload without device_map.
    if any(p.device.type == "meta" for p in _flan_model.parameters()):
        print("[models] Warning: FLAN model has 'meta' tensors; reloading on CPU to avoid errors.")
        _flan_model = AutoModelForSeq2SeqLM.from_pretrained(
            FLAN_MODEL_NAME,
            torch_dtype=torch.float32,
        )
        _flan_model.to(torch.device("cpu"))

    _flan_model.eval()
    return _flan_model, _flan_tokenizer


# --------------------------------------------------------------------
# Meditron loading
# --------------------------------------------------------------------

def load_meditron() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    Lazy-load Meditron-7B (Llama-style causal LM) on a suitable device.

    Same meta-tensor avoidance strategy as FLAN: load with appropriate
    device_map and avoid calling .to() on potentially-meta models.
    """
    global _meditron_model, _meditron_tokenizer

    if _meditron_model is not None and _meditron_tokenizer is not None:
        return _meditron_model, _meditron_tokenizer

    device = _pick_device(min_free_gb=14.0)  # Meditron-7B is larger
    print(f"[models] Loading Meditron-7B from {MEDITRON_MODEL_NAME} on {device}...")

    _meditron_tokenizer = AutoTokenizer.from_pretrained(
        MEDITRON_MODEL_NAME,
        use_fast=False,
    )

    # Important for LLaMA-style models
    if _meditron_tokenizer.pad_token_id is None:
        _meditron_tokenizer.pad_token_id = _meditron_tokenizer.eos_token_id

    if device.type == "cuda":
        _meditron_model = AutoModelForCausalLM.from_pretrained(
            MEDITRON_MODEL_NAME,
            torch_dtype=_dtype(),
            device_map="auto",
        )
    else:
        _meditron_model = AutoModelForCausalLM.from_pretrained(
            MEDITRON_MODEL_NAME,
            torch_dtype=_dtype(),
        )
        _meditron_model.to(device)

    if any(p.device.type == "meta" for p in _meditron_model.parameters()):
        print("[models] Warning: Meditron model has 'meta' tensors; reloading on CPU to avoid errors.")
        _meditron_model = AutoModelForCausalLM.from_pretrained(
            MEDITRON_MODEL_NAME,
            torch_dtype=torch.float32,
        )
        _meditron_model.to(torch.device("cpu"))

    _meditron_model.eval()
    return _meditron_model, _meditron_tokenizer


# --------------------------------------------------------------------
# Generation helpers
# --------------------------------------------------------------------

def generate_flan(
    prompt: str,
    max_new_tokens: int = 160,
    num_beams: int = 2,
) -> str:
    """
    Run FLAN-T5 on a prompt and return the decoded summary.

    Defaults are tuned to:
    - avoid repetition (no_repeat_ngram_size, repetition_penalty),
    - keep decoding deterministic (no sampling),
    - keep length reasonable per view.
    """
    model, tokenizer = load_flan()
    device = next(model.parameters()).device

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    gen_kwargs = {
        "max_new_tokens": max_new_tokens,
        "do_sample": False,
        "num_beams": num_beams,
        "repetition_penalty": 1.1,
        "no_repeat_ngram_size": 4,
        "early_stopping": True,
        "eos_token_id": tokenizer.eos_token_id,
    }

    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text.strip()


def generate_meditron(
    prompt: str,
    max_new_tokens: int = 192,
    temperature: float = 0.0,
) -> str:
    """
    Run Meditron-7B on a prompt and return the decoded continuation.
    STRIPS THE INPUT PROMPT from the output to avoid repetition.
    """
    model, tokenizer = load_meditron()
    device = next(model.parameters()).device

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048, # Increased to handle larger contexts if needed
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    input_len = inputs["input_ids"].shape[1]

    # Dynamic params based on temp
    if temperature > 0.0:
        gen_kwargs = {
            "do_sample": True,
            "temperature": max(temperature, 1e-4),
            "top_p": 0.9,
        }
    else:
        gen_kwargs = {
            "do_sample": False,
        }

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=1,
            repetition_penalty=1.1,
            no_repeat_ngram_size=4,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
            **gen_kwargs
        )

    # --- KEY FIX: Slice off the input tokens ---
    # Only decode the *new* tokens generated by the model
    generated_tokens = outputs[0][input_len:]
    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    return text.strip()

# -----------------------------------------------
# OLD VERSION OF generate_meditron FOR REFERENCE
"""
def generate_meditron(
    prompt: str,
    max_new_tokens: int = 192,
    temperature: float = 0.0,
) -> str:
    
    Run Meditron-7B on a prompt and return the decoded continuation.

    - If temperature == 0.0  -> deterministic (greedy / beam) decoding.
    - If temperature > 0.0   -> sampling with top-p and temperature.
    
    model, tokenizer = load_meditron()
    device = next(model.parameters()).device

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=768,
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    if temperature > 0.0:
        gen_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": True,
            "temperature": max(temperature, 1e-4),
            "top_p": 0.9,
            "num_beams": 1,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 4,
            "eos_token_id": tokenizer.eos_token_id,
            "pad_token_id": tokenizer.pad_token_id,
        }
    else:
        gen_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": False,
            "num_beams": 1,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 4,
            "eos_token_id": tokenizer.eos_token_id,
            "pad_token_id": tokenizer.pad_token_id,
        }

    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text.strip()
"""

================================================================================
FILE: paths.py
================================================================================
import os

# Raw data locations
MIMIC_IV_DIR = "/scratch/soham_shah/mimiciv/mimic-iv-3.1"
MIMIC_NOTES_DIR = "/scratch/soham_shah/mimiciv_note/mimiciv-clinical-notes/note"

# Processed data root
PROC_DIR = "/scratch/soham_shah/mimic_proc_data"

# Subfolders
HOSP_DIR = os.path.join(MIMIC_IV_DIR, "hosp")
ICU_DIR = os.path.join(MIMIC_IV_DIR, "icu")

HOSP_PROC_DIR = os.path.join(PROC_DIR, "hosp")
ICU_PROC_DIR = os.path.join(PROC_DIR, "icu")
NOTES_PROC_DIR = os.path.join(PROC_DIR, "notes")

# Make sure processed folders exist whenever this is imported
os.makedirs(HOSP_PROC_DIR, exist_ok=True)
os.makedirs(ICU_PROC_DIR, exist_ok=True)
os.makedirs(NOTES_PROC_DIR, exist_ok=True)

# ---- Cohort-processed data locations ----

PROC_COHORT_DIR = "/scratch/soham_shah/mimic_proc_data_cohort"

COHORT_META_DIR = os.path.join(PROC_COHORT_DIR, "meta")
HOSP_PROC_COHORT_DIR = os.path.join(PROC_COHORT_DIR, "hosp")
ICU_PROC_COHORT_DIR = os.path.join(PROC_COHORT_DIR, "icu")
NOTES_PROC_COHORT_DIR = os.path.join(PROC_COHORT_DIR, "notes")

os.makedirs(COHORT_META_DIR, exist_ok=True)
os.makedirs(HOSP_PROC_COHORT_DIR, exist_ok=True)
os.makedirs(ICU_PROC_COHORT_DIR, exist_ok=True)
os.makedirs(NOTES_PROC_COHORT_DIR, exist_ok=True)


================================================================================
FILE: prompts.py
================================================================================
from typing import Any, Dict, List


# --------------------------------------------------------------------
# Helper formatters: turn structured dicts into readable bullet blocks
# --------------------------------------------------------------------

def _format_demographics(demo: Dict[str, Any]) -> str:
    """Format demographics + admission context into a short text block."""
    if not demo:
        return (
            "Patient demographics and admission context:\n"
            "- (No demographic or admission information available.)\n\n"
        )

    age = demo.get("age") or demo.get("anchor_age") or demo.get("admission_age")
    try:
        age_int = int(age) if age is not None else None
    except Exception:
        age_int = None

    gender = demo.get("gender") or "Unknown"
    admission_type = demo.get("admission_type") or "Unknown"
    admission_location = demo.get("admission_location") or "Unknown"
    discharge_location = demo.get("discharge_location") or "Unknown"

    admit_time = (
        demo.get("admittime")
        or demo.get("admit_datetime")
        or demo.get("admitdatetime")
        or demo.get("admission_time")
    )
    discharge_time = (
        demo.get("dischtime")
        or demo.get("discharge_datetime")
        or demo.get("dischdatetime")
    )
    death_time = (
        demo.get("deathtime")
        or demo.get("death_datetime")
        or demo.get("deathdate")
    )
    hosp_expire_flag = demo.get("hospital_expire_flag")

    if age_int is not None and age_int >= 0:
        age_str = f"{age_int}"
    else:
        age_str = "Unknown"

    lines = ["Patient demographics and admission context:"]
    lines.append(f"- Age: {age_str}")
    lines.append(f"- Gender: {gender}")
    lines.append(f"- Admission type: {admission_type}")
    lines.append(f"- Admitted from: {admission_location}")
    if admit_time:
        lines.append(f"- Admission time: {admit_time}")
    lines.append(f"- Discharged to: {discharge_location}")
    if discharge_time:
        lines.append(f"- Discharge time: {discharge_time}")

    # Outcome line: discharged vs died vs unknown
    if (hosp_expire_flag == 1) or death_time:
        if death_time:
            lines.append(f"- Outcome: patient died during this admission (time: {death_time})")
        else:
            lines.append("- Outcome: patient died during this admission (death time not recorded).")
    else:
        # Only say "discharged" if we have some evidence of discharge info
        if discharge_time or discharge_location != "Unknown":
            lines.append("- Outcome: patient was discharged from this admission.")
        else:
            lines.append("- Outcome: admission outcome not recorded in this data.")

    lines.append("")
    return "\n".join(lines)


def _format_diagnoses(dx_list: List[Dict[str, Any]], max_n: int = 10) -> str:
    """Format diagnoses list into an ordered bullet block."""
    if not dx_list:
        return "Diagnoses during this hospital admission:\n- (No diagnoses recorded.)\n\n"

    # Try to sort by explicit sequence if present
    def _seq(row: Dict[str, Any]) -> Any:
        return (
            row.get("dx_seq_num")
            or row.get("sequence")
            or row.get("seq_num")
            or 0
        )

    sorted_dx = sorted(dx_list, key=_seq)
    sorted_dx = sorted_dx[:max_n]

    lines = ["Diagnoses during this hospital admission (ordered):"]
    for i, dx in enumerate(sorted_dx, start=1):
        title = (
            dx.get("dx_long_title")
            or dx.get("long_title")
            or dx.get("title")
            or dx.get("icd_code")
            or "Unknown diagnosis"
        )
        lines.append(f"{i}. {title}")
    lines.append("")
    return "\n".join(lines)


def _format_procedures(proc_list: List[Dict[str, Any]], max_n: int = 10) -> str:
    """Format procedures (usually HOSP procedures filtered to ICU window)."""
    if not proc_list:
        return (
            "Procedures performed during this admission (ICU-relevant window):\n"
            "- (No procedures recorded in the data for this window.)\n\n"
        )

    def _time_str(row: Dict[str, Any]) -> str:
        return (
            str(row.get("procedure_chartdatetime"))
            or str(row.get("charttime_str") or row.get("charttime") or "")
        )

    lines = ["Procedures performed during this admission (ICU-relevant window):"]
    for i, proc in enumerate(proc_list[:max_n], start=1):
        name = (
            proc.get("proc_long_title")
            or proc.get("procedure_name")
            or proc.get("label")
            or "Unknown procedure"
        )
        when = _time_str(proc)
        if when and when != "None":
            lines.append(f"{i}. {name} (around {when})")
        else:
            lines.append(f"{i}. {name}")
    lines.append("")
    return "\n".join(lines)


def _format_labs(lab_rows: List[Dict[str, Any]]) -> str:
    """Format aggregated lab summary rows."""
    if not lab_rows:
        return (
            "Key laboratory results and trends during the ICU stay:\n"
            "- (No ICU lab results available in the data.)\n\n"
        )

    lines = [
        "Key laboratory results and trends during the ICU stay "
        "(each bullet summarises one lab test):"
    ]
    for row in lab_rows:
        name = (
            row.get("lab_name")
            or row.get("lab_tests_label")
            or row.get("label")
            or row.get("itemid")
            or "Unknown lab test"
        )
        
        # Attempt to get unit from the new field first, fallback to old keys
        unit = (
            row.get("unit")
            or row.get("valueuom")
            or row.get("lab_tests_valueuom")
            or row.get("unitname")
            or ""
        )
        
        low = row.get("min")
        med = row.get("median")
        high = row.get("max")
        count = row.get("count")
        abn = row.get("abnormal_count")
        trend = row.get("trend")

        parts = [f"- {name}"]
        # We append unit to the numbers now, rather than the name
        
        stats_bits = []
        if med is not None:
            val_str = f"{med:.3g}"
            if unit:
                val_str += f" {unit}"
            stats_bits.append(f"median {val_str}")
            
        if low is not None and high is not None:
            range_str = f"{low:.3g}–{high:.3g}"
            if unit:
                range_str += f" {unit}"
            stats_bits.append(f"range {range_str}")
            
        # Add Trend
        if trend and trend not in ["Unknown", "Insufficient data", "Stable"]:
            stats_bits.append(f"trend: {trend.lower()}")
        elif trend == "Stable":
            stats_bits.append("trend: stable")

        if stats_bits:
            parts.append(" with " + ", ".join(stats_bits))
            
        if count is not None:
            parts.append(f"; n={int(count)}")
        if abn is not None and abn > 0:
            parts.append(f", abnormal results: {int(abn)}")
            
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)


def _format_meds(meds_rows: List[Dict[str, Any]]) -> str:
    """Format aggregated ICU medications summary rows."""
    if not meds_rows:
        return (
            "ICU medications summary (grouped by label/category):\n"
            "- (No ICU medications recorded in the data.)\n\n"
        )

    lines = ["ICU medications summary (each bullet summarises one medication label):"]
    for row in meds_rows:
        name = (
            row.get("med_name")
            or row.get("medications_label")
            or row.get("drug_name")
            or row.get("label")
            or "Unknown medication"
        )
        category = row.get("category")
        n_orders = row.get("num_orders")
        total_amount = row.get("total_amount")
        unit = row.get("unit") or ""  # <--- Get Unit
        
        start = row.get("first_start") or row.get("start_time") or row.get("start")
        end = row.get("last_end") or row.get("end_time") or row.get("end")

        parts = [f"- {name}"]
        if category:
            parts[-1] += f" [{category}]"
        if n_orders is not None:
            parts.append(f", number of orders: {int(n_orders)}")
        
        if total_amount is not None:
            amt_str = f"{float(total_amount):.3g}"
            if unit:
                amt_str += f" {unit}"  # <--- Append Unit
            parts.append(f", approximate total amount: {amt_str}")
            
        if start or end:
            parts.append(" (given")
            if start:
                parts.append(f" from {start}")
            if end:
                parts.append(f" to {end}")
            parts.append(")")
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)

"""
def _format_meds(meds_rows: List[Dict[str, Any]]) -> str:
    Format aggregated ICU medications summary rows.
    if not meds_rows:
        return (
            "ICU medications summary (grouped by label/category):\n"
            "- (No ICU medications recorded in the data.)\n\n"
        )

    lines = ["ICU medications summary (each bullet summarises one medication label):"]
    for row in meds_rows:
        name = (
            row.get("med_name")
            or row.get("medications_label")
            or row.get("drug_name")
            or row.get("label")
            or "Unknown medication"
        )
        category = row.get("category")
        n_orders = row.get("num_orders")
        total_amount = row.get("total_amount")
        start = row.get("first_start") or row.get("start_time") or row.get("start")
        end = row.get("last_end") or row.get("end_time") or row.get("end")

        parts = [f"- {name}"]
        if category:
            parts[-1] += f" [{category}]"
        if n_orders is not None:
            parts.append(f", number of orders: {int(n_orders)}")
        if total_amount is not None:
            parts.append(f", approximate total amount: {float(total_amount):.3g}")
        if start or end:
            parts.append(" (given")
            if start:
                parts.append(f" from {start}")
            if end:
                parts.append(f" to {end}")
            parts.append(")")
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)
"""

def _format_measurements(meas_rows: List[Dict[str, Any]]) -> str:
    """Format aggregated ICU measurements summary rows."""
    if not meas_rows:
        return (
            "Summarised bedside measurements and vital-sign trends during the ICU stay:\n"
            "- (No ICU measurements available in the data.)\n\n"
        )

    lines = [
        "Summarised bedside measurements and vital-sign trends during the ICU stay "
        "(each bullet summarises one measurement label):"
    ]
    for row in meas_rows:
        name = (
            row.get("measure_name")
            or row.get("measurements_label")
            or row.get("label")
            or "Unknown measurement"
        )
        low = row.get("min")
        med = row.get("median")
        high = row.get("max")
        count = row.get("count")
        
        # New fields
        unit = row.get("unit") or ""
        trend = row.get("trend")

        parts = [f"- {name}"]
        stats_bits = []
        
        # Format Median with Unit
        if med is not None:
            val_str = f"{med:.3g}"
            if unit:
                val_str += f" {unit}"
            stats_bits.append(f"median {val_str}")
        
        # Format Range with Unit
        if low is not None and high is not None:
            range_str = f"{low:.3g}–{high:.3g}"
            if unit:
                range_str += f" {unit}"
            stats_bits.append(f"range {range_str}")
            
        # Format Trend
        if trend and trend not in ["Unknown", "Insufficient data", "Stable"]:
            # Only mention trend if it's Rising or Falling to save tokens, 
            # or you can include "stable" if preferred.
            stats_bits.append(f"trend: {trend.lower()}")
        elif trend == "Stable":
            stats_bits.append("trend: stable")

        if stats_bits:
            parts.append(" with " + ", ".join(stats_bits))
        if count is not None:
            parts.append(f"; n={int(count)}")
            
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)

"""
def _format_measurements(meas_rows: List[Dict[str, Any]]) -> str:
    Format aggregated ICU measurements summary rows.
    if not meas_rows:
        return (
            "Summarised bedside measurements and vital-sign trends during the ICU stay:\n"
            "- (No ICU measurements available in the data.)\n\n"
        )

    lines = [
        "Summarised bedside measurements and vital-sign trends during the ICU stay "
        "(each bullet summarises one measurement label):"
    ]
    for row in meas_rows:
        name = (
            row.get("measure_name")
            or row.get("measurements_label")
            or row.get("label")
            or "Unknown measurement"
        )
        low = row.get("min")
        med = row.get("median")
        high = row.get("max")
        count = row.get("count")

        parts = [f"- {name}"]
        stats_bits = []
        if med is not None:
            stats_bits.append(f"median {med:.3g}")
        if low is not None and high is not None:
            stats_bits.append(f"range {low:.3g}–{high:.3g}")
        if stats_bits:
            parts.append(" with " + ", ".join(stats_bits))
        if count is not None:
            parts.append(f"; n={int(count)}")
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)
"""

def _format_outputs(outputs_rows: List[Dict[str, Any]]) -> str:
    """Format aggregated ICU output events summary rows."""
    if not outputs_rows:
        return (
            "ICU output events (urine, drains, etc.):\n"
            "- (No ICU output events recorded in the data.)\n\n"
        )

    lines = [
        "ICU output events (each bullet summarises one output label over the ICU stay):"
    ]
    for row in outputs_rows:
        name = (
            row.get("output_label")
            or row.get("outputevents_label")
            or row.get("label")
            or "Unknown output"
        )
        unit = (
            row.get("unit")
            or row.get("outputevents_valueuom")
            or row.get("valueuom")
            or ""
        )
        total = row.get("total_volume") or row.get("sum")
        low = row.get("min")
        med = row.get("median")
        high = row.get("max")
        count = row.get("count")

        parts = [f"- {name}"]
        if unit:
            parts[-1] += f" ({unit})"
        stats_bits = []
        if total is not None:
            stats_bits.append(f"total ~{float(total):.3g}")
        if med is not None:
            stats_bits.append(f"median {med:.3g}")
        if low is not None and high is not None:
            stats_bits.append(f"range {low:.3g}–{high:.3g}")
        if stats_bits:
            parts.append(" with " + ", ".join(stats_bits))
        if count is not None:
            parts.append(f"; n={int(count)}")
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)


def _format_procedureevents(proc_ev_rows: List[Dict[str, Any]]) -> str:
    """Format aggregated ICU procedureevents summary rows."""
    if not proc_ev_rows:
        return (
            "ICU bedside procedures and interventions:\n"
            "- (No ICU bedside procedures recorded in the data.)\n\n"
        )

    lines = [
        "ICU bedside procedures and interventions "
        "(each bullet summarises one procedureevents label):"
    ]
    for row in proc_ev_rows:
        label = (
            row.get("procedureevents_label")
            or row.get("label")
            or "Unknown procedure"
        )
        category = row.get("category") or row.get("procedureevents_category")
        location = row.get("location") or row.get("procedureevents_location")
        start = row.get("start") or row.get("procedureevents_startdatetime")
        end = row.get("end") or row.get("procedureevents_enddatetime")

        parts = [f"- {label}"]
        if category:
            parts[-1] += f" [{category}]"
        if location:
            parts.append(f" at {location}")
        if start or end:
            parts.append(" (performed")
            if start:
                parts.append(f" from {start}")
            if end:
                parts.append(f" to {end}")
            parts.append(")")
        lines.append("".join(parts))
    lines.append("")
    return "\n".join(lines)


# --------------------------------------------------------------------
# FLAN-style prompts (instruction-heavy)
# --------------------------------------------------------------------

def _make_flan_prompt(view_type: str, features: Dict[str, Any]) -> str:
    """Build an instruction-style prompt for FLAN-T5."""
    vt = (view_type or "").lower()

    demo_block = _format_demographics(features.get("demographics", {}))

    # 1) Admission & demographics view
    if vt == "admission":
        header = (
            "You are an ICU clinician writing a brief, factual admission note for another doctor.\n\n"
            "Task:\n"
            "- Using only the structured information below, write 2–3 sentences that describe:\n"
            "  * the patient's age and gender,\n"
            "  * how and when the patient was admitted,\n"
            "  * and whether the patient was discharged or died during this admission.\n\n"
            "Requirements:\n"
            "- Use only the information that appears in the structured data.\n"
            "- Do not guess or add clinical interpretation.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the numbers, only the structure):\n"
            "The patient is a 67-year-old woman admitted as an emergency from the emergency room on 2115-09-12. "
            "She was discharged home on 2115-09-20.\n\n"
        )
        body = "Structured data:\n" + demo_block + "\nNow write the summary:\n"
        return header + example + body

    # 2) Diagnoses + procedures view
    if vt == "dx_proc":
        dx_block = _format_diagnoses(features.get("diagnoses", []), max_n=5)
        proc_block = _format_procedures(
            features.get("icu_procedures", []) or features.get("procedures", []),
            max_n=5,
        )
        header = (
            "You are an ICU clinician writing a concise, factual summary of diagnoses and procedures "
            "for this hospital admission.\n\n"
            "Task:\n"
            "- Using only the information below, write 3–5 sentences that:\n"
            "  * first list up to 5 main diagnoses in order of importance,\n"
            "  * then list up to 5 key procedures performed during the admission.\n\n"
            "Requirements:\n"
            "- Use only the diagnosis and procedure names shown in the structured data.\n"
            "- Do not invent new diagnoses, procedures, or explanations.\n"
            "- Do not add causal statements or clinical interpretation beyond what is explicit.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the content, only the structure):\n"
            "Primary diagnoses included congestive heart failure and acute myocardial infarction of the anterolateral wall. "
            "Additional diagnoses included essential hypertension. "
            "Key procedures during this admission included coronary angiography and insertion of a pulmonary artery catheter early in the ICU stay. "
            "A tracheostomy was performed later during the admission.\n\n"
        )
        body = "Structured data:\n" + demo_block + dx_block + proc_block + "Now write the summary:\n"
        return header + example + body

    # 3) Lab events
    if vt == "labs":
        labs_block = _format_labs(features.get("labs_summary", []))
        header = (
            "You are an ICU clinician summarising the key laboratory results and trends for this ICU stay.\n\n"
            "Task:\n"
            "- Using only the laboratory information below, write 3–5 sentences that describe:\n"
            "  * which lab tests are most important,\n"
            "  * their typical values (medians and ranges),\n"
            "  * and specifically mention the 'trend' (Rising, Falling, or Stable) provided in the data for each test.\n\n"
            "Requirements:\n"
            "- Use only the tests, values, and trends shown in the structured data.\n"
            "- Do not invent new lab tests or values.\n"
            "- Do not provide detailed pathophysiological explanations; stay factual.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the numbers, only the structure):\n"
            "Key laboratory tests included creatinine with a median of 1.4 mg/dL (range 0.9–2.3) and a rising trend. "
            "Hemoglobin was repeatedly low with a median of 9.2 g/dL (range 8.5–10.0) and remained stable. "
            "Sodium levels were relatively stable around a median of 138 mmol/L.\n\n"
        )
        body = "Structured data:\n" + labs_block + "Now write the lab summary:\n"
        return header + example + body

    # 4) Medications
    if vt == "meds":
        meds_block = _format_meds(features.get("meds_summary", []))
        header = (
            "You are an ICU clinician summarising the medication course for this ICU stay.\n\n"
            "Task:\n"
            "- Using only the medication information below, write 3–4 sentences that:\n"
            "  * highlight the most important medications in each category,\n"
            "  * mention total amounts (with units) and time periods,\n"
            "  * and describe the overall therapeutic strategy (for example, antibiotics for infection or vasopressors for shock) "
            "without inventing new drugs.\n\n"
            "Requirements:\n"
            "- Use only the medication names, categories, and dates shown in the structured data.\n"
            "- Do not invent drug names, doses, or durations.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the drug names, only the structure):\n"
            "Key medications included norepinephrine given repeatedly from 2115-09-12 to 2115-09-14, "
            "and piperacillin–tazobactam as an antibiotic from 2115-09-12 to 2115-09-18.\n\n"
        )
        body = "Structured data:\n" + meds_block + "Now write the medication summary:\n"
        return header + example + body

    # 5) Measurements / vitals
    if vt == "measurements":
        meas_block = _format_measurements(features.get("measurements_summary", []))
        header = (
            "You are an ICU clinician summarising vital signs and other bedside measurements for this ICU stay.\n\n"
            "Task:\n"
            "- Using only the measurement information below, write 3–5 sentences that describe:\n"
            "  * the typical values (medians and ranges) for key measurements,\n"
            "  * and explicitly mention the 'trend' (Rising, Falling, or Stable) for each vital sign.\n\n"
            "Requirements:\n"
            "- Use only the measurement labels, values, and trends shown in the structured data.\n"
            "- Do not label values as 'normal' or 'abnormal' unless this is explicitly encoded; just describe the numbers.\n"
            "- Do not invent additional measurements or time periods.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the numbers, only the structure):\n"
            "During the ICU stay, oxygen saturation was generally well maintained with a median of 96% (range 90–99%). "
            "Heart rate showed a rising trend with a median of 92 beats per minute (range 70–130). "
            "Systolic blood pressure was stable with a median of 110 mmHg. "
            "Respiratory rate remained relatively stable around a median of 18 breaths per minute.\n\n"
        )
        body = "Structured data:\n" + meas_block + "Now write the measurements summary:\n"
        return header + example + body

    # 6) Output events
    if vt == "outputs":
        outputs_block = _format_outputs(features.get("outputs_summary", []))
        header = (
            "You are an ICU clinician summarising fluid outputs for this ICU stay.\n\n"
            "Task:\n"
            "- Using only the output information below, write 3–5 sentences that describe:\n"
            "  * the main types of outputs (for example, urine via Foley catheter, drain output),\n"
            "  * approximate total volumes and time windows when available,\n"
            "  * and simple trends such as stable, increasing, or decreasing outputs.\n\n"
            "Requirements:\n"
            "- Use only the output labels, units, and values shown in the structured data.\n"
            "- Do not invent additional fluids, volumes, or trends.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the numbers, only the structure):\n"
            "Urine output via Foley catheter totalled about 1800 mL between 2115-09-12 and 2115-09-13 and remained relatively stable. "
            "Chest drain output was around 600 mL from 2115-09-12 to 2115-09-14 and tended to decrease over time. "
            "Nasogastric output was modest with lower volumes and no clear trend. "
            "Overall, fluid outputs were monitored closely with stable urinary output and gradually declining drain output.\n\n"
        )
        body = "Structured data:\n" + outputs_block + "Now write the output events summary:\n"
        return header + example + body

    # 7) ICU procedureevents
    if vt == "procedureevents":
        proc_ev_block = _format_procedureevents(features.get("procedureevents_summary", []))
        header = (
            "You are an ICU clinician summarising bedside procedures and interventions during this ICU stay.\n\n"
            "Task:\n"
            "- Using only the procedure information below, write 3–5 sentences that describe:\n"
            "  * the types of procedures performed,\n"
            "  * where they were performed (location),\n"
            "  * and the approximate timing of these procedures.\n\n"
            "Requirements:\n"
            "- Use only the procedure labels, categories, locations, and times shown in the structured data.\n"
            "- Do not invent new procedures, indications, or complications.\n"
            "- Do not mention 'tables' or 'structured data'.\n\n"
        )
        example = (
            "Example output style (do NOT copy the content, only the structure):\n"
            "ICU bedside procedures included placement of a 20-gauge peripheral line in the left forearm on 2115-09-12. "
            "Chest X-rays were obtained in the ICU on 2115-09-12. "
            "A paracentesis was performed later during the ICU stay. "
            "These procedures were performed at the bedside in the ICU.\n\n"
        )
        body = "Structured data:\n" + proc_ev_block + "Now write the procedureevents summary:\n"
        return header + example + body

# --------------------------------------------------------------------
# Meditron-style prompts (shorter, completion-oriented)
# --------------------------------------------------------------------

def _make_meditron_prompt(view_type: str, features: Dict[str, Any]) -> str:
    """Build a Llama-2 style instruction prompt for Meditron."""
    vt = (view_type or "").lower()
    demo_block = _format_demographics(features.get("demographics", {}))

    # Base instruction wrapper
    def wrap_inst(instruction, data_content):
        return (
            "[INST] You are a helpful clinical assistant. "
            f"{instruction}\n\n"
            "Structured Data:\n"
            f"{data_content}\n"
            "[/INST]\n"
            "Summary:"
        )

    if vt == "admission":
        return wrap_inst(
            instruction=(
                "Using the structured admission data below, write 2–3 sentences describing "
                "the patient's age, gender, admission context, and whether they were discharged or died. "
                "Do not invent clinical details."
            ),
            data_content=f"{demo_block}"
        )

    if vt == "dx_proc":
        dx_block = _format_diagnoses(features.get("diagnoses", []), max_n=5)
        proc_block = _format_procedures(
            features.get("icu_procedures", []) or features.get("procedures", []),
            max_n=5,
        )
        return wrap_inst(
            instruction=(
                "Using the structured data below, write 3–5 sentences describing the main diagnoses "
                "and key procedures in the order given. Do not add extra interpretation."
            ),
            data_content=f"{demo_block}{dx_block}{proc_block}"
        )

    if vt == "labs":
        labs_block = _format_labs(features.get("labs_summary", []))
        return wrap_inst(
            instruction=(
                "Using the lab tests below, write 3–5 sentences describing key tests, "
                "their median values/ranges, and the calculated trend (Rising/Falling/Stable)."
            ),
            data_content=f"{labs_block}"
        )

    if vt == "meds":
        meds_block = _format_meds(features.get("meds_summary", []))
        return wrap_inst(
            instruction=(
                "Using the medication list below, write 3–4 sentences highlighting the most important "
                "medications in each category and their approximate time periods."
            ),
            data_content=f"{meds_block}"
        )

    if vt == "measurements":
        meas_block = _format_measurements(features.get("measurements_summary", []))
        return wrap_inst(
            instruction=(
                "Using the measurements below, write 3–5 sentences describing the main vitals, "
                "their median values/ranges, and their trend (Rising/Falling/Stable). "
                "Do not use labels like 'normal' unless explicitly shown."
            ),
            data_content=f"{meas_block}"
        )

    if vt == "outputs":
        outputs_block = _format_outputs(features.get("outputs_summary", []))
        return wrap_inst(
            instruction=(
                "Using the output events below (urine, drains, etc.), write 3–5 sentences describing "
                "the main output types, total volumes, and time windows."
            ),
            data_content=f"{outputs_block}"
        )

    if vt == "procedureevents":
        proc_ev_block = _format_procedureevents(features.get("procedureevents_summary", []))
        return wrap_inst(
            instruction=(
                "Using the ICU bedside procedures below, write 3–5 sentences describing the procedures "
                "by category, mentioning locations and approximate dates."
            ),
            data_content=f"{proc_ev_block}"
        )

    # Default fallback to dx_proc style if view_type is unknown
    return _make_meditron_prompt("dx_proc", features)

# --------------------------------------------------------------------
# Public API
# --------------------------------------------------------------------

def make_prompt(view_type: str, features: Dict[str, Any], model_name: str = "") -> str:
    """
    Build a text prompt for a given view and model.

    model_name:
      - "flan" or containing "flan"/"t5"  -> FLAN-style instruction prompt
      - "meditron"                        -> Meditron clinical-note style prompt
    """
    name = (model_name or "").lower()
    if "meditron" in name:
        return _make_meditron_prompt(view_type, features)
    if "flan" in name or "t5" in name:
        return _make_flan_prompt(view_type, features)
    # Default to FLAN style if unknown
    return _make_flan_prompt(view_type, features)

================================================================================
FILE: scripts/export_three_stays_jsonl.py
================================================================================
import os
import sys
import json
from typing import Any, Dict

import pandas as pd

# -------------------------------------------------------------------
# Make sure the project root (/home/soham_shah/mimic_llm) is on sys.path
# -------------------------------------------------------------------
THIS_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(THIS_DIR)  # one level up from scripts/
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from features import load_all_tables_for_stay


# Hard-coded list of stay_ids you want to export
STAY_IDS = [
    38657298,
    35527336,
    35517464,
]


def df_to_records(obj: Any):
    """Convert a DataFrame to a list of dicts; leave other types unchanged."""
    if isinstance(obj, pd.DataFrame):
        return obj.to_dict(orient="records")
    return obj


def build_export_record(stay_id: int) -> Dict[str, Any]:
    """
    Load all cohort tables for a single stay_id and package them into
    one JSON-serialisable dict.
    """
    stay_data = load_all_tables_for_stay(stay_id)

    record: Dict[str, Any] = {"stay_id": int(stay_id)}

    # Copy everything from stay_data, converting DataFrames to plain records
    for key, value in stay_data.items():
        record[key] = df_to_records(value)

    return record


def main():
    out_dir = os.path.join(PROJECT_ROOT, "exports")
    os.makedirs(out_dir, exist_ok=True)

    out_path = os.path.join(out_dir, "three_stays_actual_data.jsonl")

    print(f"Exporting stay data for {len(STAY_IDS)} stays...")
    print(f"Output file: {out_path}")

    with open(out_path, "w", encoding="utf-8") as f:
        for stay_id in STAY_IDS:
            print(f"  - Processing stay_id={stay_id} ...")
            record = build_export_record(int(stay_id))
            # default=str handles timestamps and other non-JSON-native types
            line = json.dumps(record, default=str)
            f.write(line + "\n")

    print("Done.")
    print("You can now download or share:", out_path)


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/export_three_stays_txt.py
================================================================================
import os
import sys
from typing import Any

import pandas as pd

# -------------------------------------------------------------------
# Ensure project root (/home/soham_shah/mimic_llm) is on sys.path
# -------------------------------------------------------------------
THIS_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(THIS_DIR)  # one level up from scripts/
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from features import load_all_tables_for_stay


# Hard-coded list of stay_ids you want to export
STAY_IDS = [
    38657298,
    35527336,
    35517464,
]


TABLE_SEP = "-" * 100
STAY_SEP = "=" * 100


def safe_str(val: Any) -> str:
    """Convert values to string safely (handles timestamps, etc.)."""
    try:
        return str(val)
    except Exception:
        return repr(val)


def write_table_block(f, key: str, value: Any):
    """
    Print one table or object for a stay into the file, showing
    ALL columns and ALL rows, with clear formatting.
    """
    f.write(TABLE_SEP + "\n")
    f.write(f"TABLE: {key}\n")

    if isinstance(value, pd.DataFrame):
        n_rows, n_cols = value.shape
        f.write(f"(DataFrame with {n_rows} rows x {n_cols} columns)\n\n")

        # Show all columns explicitly
        cols = [str(c) for c in value.columns]
        f.write("COLUMNS:\n")
        f.write("  " + ", ".join(cols) + "\n\n")

        if n_rows == 0:
            f.write("[NO ROWS]\n")
        else:
            # Convert to list-of-dicts and print each row clearly
            records = value.to_dict(orient="records")
            for i, row in enumerate(records):
                f.write(f"ROW {i}:\n")
                for col in cols:
                    v = row.get(col)
                    f.write(f"  {col}: {safe_str(v)}\n")
                f.write("\n")
    else:
        f.write(f"(Non-DataFrame object of type {type(value).__name__})\n\n")
        # For non-DataFrame objects (dicts, strings, etc.), just dump repr
        f.write(safe_str(value))
        f.write("\n")

    f.write(TABLE_SEP + "\n\n")


def main():
    out_dir = os.path.join(PROJECT_ROOT, "exports")
    os.makedirs(out_dir, exist_ok=True)

    out_path = os.path.join(out_dir, "three_stays_actual_data.txt")

    print(f"Exporting human-readable data for {len(STAY_IDS)} stays...")
    print(f"Output file: {out_path}")

    with open(out_path, "w", encoding="utf-8") as f:
        for stay_id in STAY_IDS:
            stay_id = int(stay_id)
            print(f"  - Processing stay_id={stay_id} ...")
            stay_data = load_all_tables_for_stay(stay_id)

            # Big header for this stay
            f.write(STAY_SEP + "\n")
            f.write(f"STAY_ID = {stay_id}\n")
            f.write(STAY_SEP + "\n\n")

            # One clearly separated block per table / key
            for key, value in stay_data.items():
                write_table_block(f, key, value)

            f.write("\n\n")

    print("Done.")
    print("You can now download or share:", out_path)


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_diagnoses_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_PROC_DIR, COHORT_META_DIR, HOSP_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    dx_path = os.path.join(HOSP_PROC_DIR, "diagnoses_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading diagnoses from:", dx_path)

    cohort = pd.read_parquet(cohort_path)
    dx = pd.read_parquet(dx_path)

    hadm_ids = set(cohort["hadm_id"].unique())
    print("Number of cohort hadm_ids:", len(hadm_ids))

    dx_cohort = dx[dx["hadm_id"].isin(hadm_ids)].copy()

    out_path = os.path.join(HOSP_PROC_COHORT_DIR, "diagnoses_clean_icu_250.parquet")
    dx_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered diagnoses to: {out_path}")
    print(f"Rows: {len(dx_cohort)}, Cols: {len(dx_cohort.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_discharge_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import NOTES_PROC_DIR, COHORT_META_DIR, NOTES_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    discharge_path = os.path.join(NOTES_PROC_DIR, "discharge_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading discharge notes from:", discharge_path)

    cohort = pd.read_parquet(cohort_path)
    discharge = pd.read_parquet(discharge_path)

    hadm_ids = set(cohort["hadm_id"].unique())
    print("Number of cohort hadm_ids:", len(hadm_ids))

    df = discharge[discharge["hadm_id"].isin(hadm_ids)].copy()

    # Ensure charttime exists and is datetime
    if "charttime" in df.columns:
        df["charttime"] = pd.to_datetime(df["charttime"], errors="coerce")
        # sort so we can pick the latest note per hadm_id
        df = df.sort_values(["hadm_id", "charttime"])
        df_latest = df.groupby("hadm_id").tail(1).copy()
    else:
        # If no charttime, just pick the last row per hadm_id by index
        df_latest = df.sort_values(["hadm_id"]).groupby("hadm_id").tail(1).copy()

    out_path = os.path.join(NOTES_PROC_COHORT_DIR, "discharge_clean_icu_250.parquet")
    df_latest.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered discharge notes to: {out_path}")
    print(f"Rows: {len(df_latest)}, Cols: {len(df_latest.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_icustays_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_PROC_DIR, COHORT_META_DIR, ICU_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    icu_path = os.path.join(ICU_PROC_DIR, "icustays_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading icustays from:", icu_path)

    cohort = pd.read_parquet(cohort_path)
    icu = pd.read_parquet(icu_path)

    stay_ids = set(cohort["stay_id"].unique())
    print("Number of cohort stay_ids:", len(stay_ids))

    icu_cohort = icu[icu["stay_id"].isin(stay_ids)].copy()

    out_path = os.path.join(ICU_PROC_COHORT_DIR, "icustays_clean_icu_250.parquet")
    icu_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered icustays to: {out_path}")
    print(f"Rows: {len(icu_cohort)}, Cols: {len(icu_cohort.columns)}")


if __name__ == "__main__":
    main()


================================================================================
FILE: scripts/filter_lab_tests_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_PROC_DIR, COHORT_META_DIR, HOSP_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    labs_path = os.path.join(HOSP_PROC_DIR, "lab_tests_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading lab_tests from:", labs_path)

    cohort = pd.read_parquet(cohort_path)
    labs = pd.read_parquet(labs_path)

    cohort_small = cohort[["hadm_id", "stay_id", "intime", "outtime"]].copy()
    cohort_small["intime"] = pd.to_datetime(cohort_small["intime"], errors="coerce")
    cohort_small["outtime"] = pd.to_datetime(cohort_small["outtime"], errors="coerce")

    labs["lab_tests_charttime"] = pd.to_datetime(
        labs["lab_tests_charttime"], errors="coerce"
    )

    # Join labs to cohort by hadm_id
    merged = labs.merge(
        cohort_small,
        on="hadm_id",
        how="inner",
        validate="m:m"
    )

    # Filter to ICU window
    mask = (
        merged["lab_tests_charttime"] >= merged["intime"]
    ) & (
        merged["lab_tests_charttime"] <= merged["outtime"]
    )

    labs_window = merged[mask].copy()

    out_path = os.path.join(HOSP_PROC_COHORT_DIR, "lab_tests_clean_icu_250.parquet")
    labs_window.to_parquet(out_path, index=False)

    print(f"Saved ICU-window lab tests to: {out_path}")
    print(f"Rows: {len(labs_window)}, Cols: {len(labs_window.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_measurements_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_PROC_DIR, COHORT_META_DIR, ICU_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    meas_path = os.path.join(ICU_PROC_DIR, "measurements_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading measurements from:", meas_path)

    cohort = pd.read_parquet(cohort_path)
    measurements = pd.read_parquet(meas_path)

    stay_ids = set(cohort["stay_id"].unique())
    print("Number of cohort stay_ids:", len(stay_ids))

    meas_cohort = measurements[measurements["stay_id"].isin(stay_ids)].copy()

    out_path = os.path.join(ICU_PROC_COHORT_DIR, "measurements_clean_icu_250.parquet")
    meas_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered measurements to: {out_path}")
    print(f"Rows: {len(meas_cohort)}, Cols: {len(meas_cohort.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_medications_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_PROC_DIR, COHORT_META_DIR, ICU_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    med_path = os.path.join(ICU_PROC_DIR, "medications_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading medications from:", med_path)

    cohort = pd.read_parquet(cohort_path)
    meds = pd.read_parquet(med_path)

    stay_ids = set(cohort["stay_id"].unique())
    print("Number of cohort stay_ids:", len(stay_ids))

    meds_cohort = meds[meds["stay_id"].isin(stay_ids)].copy()

    out_path = os.path.join(ICU_PROC_COHORT_DIR, "medications_clean_icu_250.parquet")
    meds_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered medications to: {out_path}")
    print(f"Rows: {len(meds_cohort)}, Cols: {len(meds_cohort.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_outputevents_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_PROC_DIR, COHORT_META_DIR, ICU_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    out_path_full = os.path.join(ICU_PROC_DIR, "outputevents_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading outputevents from:", out_path_full)

    cohort = pd.read_parquet(cohort_path)
    outputs = pd.read_parquet(out_path_full)

    stay_ids = set(cohort["stay_id"].unique())
    print("Number of cohort stay_ids:", len(stay_ids))

    outputs_cohort = outputs[outputs["stay_id"].isin(stay_ids)].copy()

    out_path = os.path.join(ICU_PROC_COHORT_DIR, "outputevents_clean_icu_250.parquet")
    outputs_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered outputevents to: {out_path}")
    print(f"Rows: {len(outputs_cohort)}, Cols: {len(outputs_cohort.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_patients_admissions_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_PROC_DIR, COHORT_META_DIR, HOSP_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    patadm_path = os.path.join(HOSP_PROC_DIR, "patients_admissions_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading patients_admissions from:", patadm_path)

    cohort = pd.read_parquet(cohort_path)
    patadm = pd.read_parquet(patadm_path)

    hadm_ids = set(cohort["hadm_id"].unique())
    print("Number of cohort hadm_ids:", len(hadm_ids))

    patadm_cohort = patadm[patadm["hadm_id"].isin(hadm_ids)].copy()

    out_path = os.path.join(HOSP_PROC_COHORT_DIR, "patients_admissions_clean_icu_250.parquet")
    patadm_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered patients_admissions to: {out_path}")
    print(f"Rows: {len(patadm_cohort)}, Cols: {len(patadm_cohort.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_procedureevents_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_PROC_DIR, COHORT_META_DIR, ICU_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    proc_path = os.path.join(ICU_PROC_DIR, "procedureevents_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading procedureevents from:", proc_path)

    cohort = pd.read_parquet(cohort_path)
    procs = pd.read_parquet(proc_path)

    stay_ids = set(cohort["stay_id"].unique())
    print("Number of cohort stay_ids:", len(stay_ids))

    procs_cohort = procs[procs["stay_id"].isin(stay_ids)].copy()

    out_path = os.path.join(ICU_PROC_COHORT_DIR, "procedureevents_clean_icu_250.parquet")
    procs_cohort.to_parquet(out_path, index=False)

    print(f"Saved cohort-filtered procedureevents to: {out_path}")
    print(f"Rows: {len(procs_cohort)}, Cols: {len(procs_cohort.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/filter_procedures_to_cohort.py
================================================================================
import os
import sys
import pandas as pd

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_PROC_DIR, COHORT_META_DIR, HOSP_PROC_COHORT_DIR


def main():
    cohort_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    proc_path = os.path.join(HOSP_PROC_DIR, "procedures_clean.parquet")

    print("Reading cohort from:", cohort_path)
    print("Reading procedures from:", proc_path)

    cohort = pd.read_parquet(cohort_path)
    procs = pd.read_parquet(proc_path)

    # Keep only needed columns from cohort
    cohort_small = cohort[["hadm_id", "stay_id", "intime", "outtime"]].copy()

    # Ensure times are datetime
    cohort_small["intime"] = pd.to_datetime(cohort_small["intime"], errors="coerce")
    cohort_small["outtime"] = pd.to_datetime(cohort_small["outtime"], errors="coerce")

    procs["procedure_chartdatetime"] = pd.to_datetime(
        procs["procedure_chartdatetime"], errors="coerce"
    )

    # Join procedures to cohort by hadm_id to get ICU windows + stay_id
    merged = procs.merge(
        cohort_small,
        on="hadm_id",
        how="inner",          # keep only admissions in cohort
        validate="m:m"
    )

    # Filter to ICU window
    mask = (
        merged["procedure_chartdatetime"] >= merged["intime"]
    ) & (
        merged["procedure_chartdatetime"] <= merged["outtime"]
    )

    procs_window = merged[mask].copy()

    out_path = os.path.join(HOSP_PROC_COHORT_DIR, "procedures_clean_icu_250.parquet")
    procs_window.to_parquet(out_path, index=False)

    print(f"Saved ICU-window procedures to: {out_path}")
    print(f"Rows: {len(procs_window)}, Cols: {len(procs_window.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_cohort_icu_250.py
================================================================================
#!/usr/bin/env python

"""
make_cohort_icu_250.py

Build a 250-stay ICU cohort with the following rules:

1. Only include ICU stays whose corresponding hadm_id has a discharge
   summary, and specifically where the number of discharge summaries == 1.
2. Hard-include the following 3 stay_ids in the cohort:
   - 38657298
   - 35527336
   - 35517464
3. Fill the remaining slots with a random sample from the rest.

Output:
- Writes cohort_icu_250.parquet to COHORT_META_DIR.
"""

import os
import sys
import pandas as pd

# --- Wire up project root and paths.py ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_PROC_DIR, NOTES_PROC_DIR, COHORT_META_DIR  # type: ignore


FORCED_STAY_IDS = [38657298, 35527336, 35517464]
TARGET_COHORT_SIZE = 250
RANDOM_SEED = 42


def main():
    # 1. Load full ICU stays table (processed, not yet cohort-filtered)
    icu_path = os.path.join(ICU_PROC_DIR, "icustays_clean.parquet")
    print(f"Reading ICU stays from: {icu_path}")
    icu = pd.read_parquet(icu_path)

    if "stay_id" not in icu.columns or "hadm_id" not in icu.columns:
        raise ValueError("icustays_clean.parquet must contain 'stay_id' and 'hadm_id' columns.")

    # 2. Load full discharge notes table
    disc_path = os.path.join(NOTES_PROC_DIR, "discharge_clean.parquet")
    print(f"Reading discharge notes from: {disc_path}")
    disc = pd.read_parquet(disc_path)

    if "hadm_id" not in disc.columns or "note_id" not in disc.columns:
        raise ValueError("discharge_clean.parquet must contain 'hadm_id' and 'note_id' columns.")

    # 3. Keep only hadm_id with exactly 1 discharge note
    notes_per_hadm = (
        disc.groupby("hadm_id")["note_id"]
        .nunique()
        .rename("n_discharge_notes")
    )

    print("\n=== Discharge summary counts per hadm_id ===")
    print(f"Total hadm_id with any discharge note      : {len(notes_per_hadm)}")
    print(f"Max discharge notes per hadm_id            : {notes_per_hadm.max()}")

    hadm_with_exactly_one_note = notes_per_hadm[notes_per_hadm == 1].index
    print(f"hadm_id with exactly 1 discharge note      : {len(hadm_with_exactly_one_note)}")

    # 4. Filter ICU stays to only those admissions with exactly 1 discharge note
    icu_filtered = icu[icu["hadm_id"].isin(hadm_with_exactly_one_note)].copy()

    print("\n=== ICU stays after filtering by discharge summaries ===")
    print(f"Total ICU stays before filter              : {icu['stay_id'].nunique()}")
    print(f"ICU stays with hadm_id having 1 discharge  : {icu_filtered['stay_id'].nunique()}")

    # 5. Ensure forced stay_ids are present and satisfy the filter
    all_stay_ids = set(icu["stay_id"].unique())
    missing_forced = [s for s in FORCED_STAY_IDS if s not in all_stay_ids]

    if missing_forced:
        raise ValueError(
            f"The following forced stay_id(s) do not exist in icustays_clean: {missing_forced}"
        )

    forced_rows = icu_filtered[icu_filtered["stay_id"].isin(FORCED_STAY_IDS)].copy()
    missing_in_filtered = sorted(set(FORCED_STAY_IDS) - set(forced_rows["stay_id"]))

    if missing_in_filtered:
        # This means those stays exist, but their hadm_id does NOT have exactly 1 discharge note.
        # Since your requirement is "only hadm_id with discharge summaries (=1)", we stop here.
        raise ValueError(
            "The following forced stay_id(s) do not belong to admissions with exactly 1 "
            f"discharge summary and therefore cannot be included under the current rules: "
            f"{missing_in_filtered}"
        )

    print("\n=== Forced stays check ===")
    print(f"Forced stay_ids                            : {FORCED_STAY_IDS}")
    print(f"Forced stay_ids present after filter       : {sorted(forced_rows['stay_id'].unique().tolist())}")

    n_forced = len(forced_rows)
    if n_forced != len(FORCED_STAY_IDS):
        raise ValueError(
            f"Expected {len(FORCED_STAY_IDS)} forced stays in filtered set, "
            f"but found {n_forced}."
        )

    # 6. Sample the remaining stays to reach TARGET_COHORT_SIZE
    n_to_sample = TARGET_COHORT_SIZE - n_forced
    print(f"\nWe need to sample {n_to_sample} additional stays to reach {TARGET_COHORT_SIZE}.")

    remaining = icu_filtered[~icu_filtered["stay_id"].isin(FORCED_STAY_IDS)].copy()
    n_available = remaining["stay_id"].nunique()

    print(f"Available non-forced stays after filter    : {n_available}")

    if n_available < n_to_sample:
        raise ValueError(
            f"Not enough remaining stays to sample {n_to_sample}. "
            f"Only {n_available} available after applying filters."
        )

    sampled = (
        remaining
        .drop_duplicates(subset=["stay_id"])
        .sample(n=n_to_sample, random_state=RANDOM_SEED)
        .copy()
    )

    # 7. Build final cohort: forced rows + sampled rows
    cohort = pd.concat([forced_rows, sampled], ignore_index=True)

    # Optional: enforce one row per stay_id
    cohort = cohort.drop_duplicates(subset=["stay_id"]).copy()

    print("\n=== Final cohort summary ===")
    print(f"Total rows in cohort                       : {len(cohort)}")
    print(f"Unique stay_id in cohort                   : {cohort['stay_id'].nunique()}")
    print(f"Unique hadm_id in cohort                   : {cohort['hadm_id'].nunique()}")

    # Sort for readability (by stay_id)
    cohort = cohort.sort_values("stay_id").reset_index(drop=True)

    # 8. Save to COHORT_META_DIR
    os.makedirs(COHORT_META_DIR, exist_ok=True)
    out_path = os.path.join(COHORT_META_DIR, "cohort_icu_250.parquet")
    cohort.to_parquet(out_path, index=False)

    print(f"\nCohort saved to: {out_path}")
    print("Done.")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_diagnoses_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))      # .../mimic_llm/scripts
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)                   # .../mimic_llm

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_DIR, HOSP_PROC_DIR


def main():
    # 1. Define input paths for diagnoses and dictionary
    diag_path = os.path.join(HOSP_DIR, "diagnoses_icd.csv.gz")
    diag_dict_path = os.path.join(HOSP_DIR, "d_icd_diagnoses.csv.gz")

    print("Reading diagnoses from:", diag_path)
    print("Reading diagnoses dictionary from:", diag_dict_path)

    # 2. Read raw tables
    diagnoses = pd.read_csv(diag_path, compression="gzip")
    diag_dict = pd.read_csv(diag_dict_path, compression="gzip")

    # 3. Optional: ensure dictionary has unique (icd_code, icd_version)
    if {"icd_code", "icd_version"}.issubset(diag_dict.columns):
        diag_dict = diag_dict.drop_duplicates(subset=["icd_code", "icd_version"])

    # 4. Merge to attach long_title (human-readable diagnosis)
    df = diagnoses.merge(
        diag_dict,
        on=["icd_code", "icd_version"],
        how="left",
        validate="m:1"  # many diagnoses rows to 1 dictionary row
    )

    # 5. Rename columns to make their purpose clear
    #    seq_num = diagnosis ordering within an admission
    if "seq_num" in df.columns:
        df = df.rename(columns={"seq_num": "dx_seq_num"})

    if "long_title" in df.columns:
        df = df.rename(columns={"long_title": "dx_long_title"})

    # 6. Optionally drop raw code columns (we have dx_long_title now)
    cols_to_drop = [c for c in ["icd_code", "icd_version"] if c in df.columns]
    if cols_to_drop:
        df = df.drop(columns=cols_to_drop)

    # 7. Save to processed folder as Parquet
    out_path = os.path.join(HOSP_PROC_DIR, "diagnoses_clean.parquet")
    df.to_parquet(out_path, index=False)

    print(f"Saved cleaned diagnoses table to: {out_path}")
    print(f"Rows: {len(df)}, Columns: {len(df.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_discharge_notes_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))      # .../mimic_llm/scripts
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)                   # .../mimic_llm

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import MIMIC_NOTES_DIR, NOTES_PROC_DIR


def main():
    # 1. Define input path
    discharge_path = os.path.join(MIMIC_NOTES_DIR, "discharge.csv.gz")
    print("Reading discharge notes from:", discharge_path)

    # 2. Read raw discharge notes
    df = pd.read_csv(discharge_path, compression="gzip")

    # 3. Convert time columns to datetime
    for col in ["charttime", "storetime"]:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce")

    # 4. Create convenience date columns (when note was charted)
    if "charttime" in df.columns:
        df["discharge_note_date"] = df["charttime"].dt.date

    # 5. (Optional) strip text whitespace
    if "text" in df.columns:
        df["text"] = df["text"].astype(str).str.strip()

    # 6. Save to processed folder
    out_path = os.path.join(NOTES_PROC_DIR, "discharge_clean.parquet")
    df.to_parquet(out_path, index=False)

    print(f"Saved cleaned discharge notes to: {out_path}")
    print(f"Rows: {len(df)}, Columns: {len(df.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_icustays_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))      # .../mimic_llm/scripts
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)                   # .../mimic_llm

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_DIR, ICU_PROC_DIR


def main():
    # 1. Define input path
    icustays_path = os.path.join(ICU_DIR, "icustays.csv.gz")
    print("Reading icustays from:", icustays_path)

    # 2. Read raw icustays table
    df = pd.read_csv(icustays_path, compression="gzip")

    # 3. Convert intime and outtime to datetimes
    for col in ["intime", "outtime"]:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce")

    # 4. Create convenience date columns
    if "intime" in df.columns:
        df["icu_intake_date"] = df["intime"].dt.date
    if "outtime" in df.columns:
        df["icu_out_date"] = df["outtime"].dt.date

    # 5. Optionally ensure LOS is float (length of stay in days)
    if "los" in df.columns:
        df["los"] = pd.to_numeric(df["los"], errors="coerce")

    # 6. Save to processed folder
    out_path = os.path.join(ICU_PROC_DIR, "icustays_clean.parquet")
    df.to_parquet(out_path, index=False)

    print(f"Saved cleaned icustays table to: {out_path}")
    print(f"Rows: {len(df)}, Columns: {len(df.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_lab_tests_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))      # .../mimic_llm/scripts
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)                   # .../mimic_llm

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_DIR, HOSP_PROC_DIR


def main():
    # 1. Define input paths
    labevents_path = os.path.join(HOSP_DIR, "labevents.csv.gz")
    labitems_path = os.path.join(HOSP_DIR, "d_labitems.csv.gz")

    print("Reading labevents from:", labevents_path)
    print("Reading d_labitems from:", labitems_path)

    # 2. Read labitems dictionary (small)
    labitems = pd.read_csv(labitems_path, compression="gzip")
    # ensure unique itemid in dictionary
    if "itemid" in labitems.columns:
        labitems = labitems.drop_duplicates(subset=["itemid"])

    # 3. Read labevents
    # NOTE: This is a very large table (~158M rows). This may use a lot of memory.
    # We read a subset of columns that we actually need, which is allowed
    # under your "dropping unnecessary columns is okay" rule.
    usecols = [
        "labevent_id",
        "subject_id",
        "hadm_id",
        "specimen_id",
        "itemid",
        "order_provider_id",
        "charttime",
        "storetime",
        "value",
        "valuenum",
        "valueuom",
        "ref_range_lower",
        "ref_range_upper",
        "flag",
        "priority",
        # if there are extra columns in labevents, they will just be ignored
    ]

    labevents = pd.read_csv(
        labevents_path,
        compression="gzip",
        usecols=lambda c: c in usecols  # keep the ones above, drop others if any
    )

    print("Raw labevents shape:", labevents.shape)
    print("Raw d_labitems shape:", labitems.shape)

    # 4. Merge labevents with labitems to attach labels, fluid, category, etc.
    merged = labevents.merge(
        labitems,
        on="itemid",
        how="left",        # keep ALL labevents rows
        validate="m:1"     # many labevents to 1 labitems row
    )

    # 5. Drop columns we don't need (column-level only, no row drops)
    cols_to_drop = [
        "ref_range_lower",
        "ref_range_upper",
        "priority",
        "order_provider_id",
        "storetime",
        "specimen_id",
        "labevent_id",
        "itemid",          # keep description from labitems instead
    ]
    existing_drop_cols = [c for c in cols_to_drop if c in merged.columns]
    lab_tests = merged.drop(columns=existing_drop_cols)

    # 6. Convert charttime to datetime and add date/time columns
    if "charttime" in lab_tests.columns:
        lab_tests["charttime"] = pd.to_datetime(lab_tests["charttime"], errors="coerce")
        lab_tests["date"] = lab_tests["charttime"].dt.date
        lab_tests["time"] = lab_tests["charttime"].dt.time

    # 7. Rename 'flag' -> 'warning' and make it binary (1 = abnormal, 0 otherwise)
    if "flag" in lab_tests.columns:
        lab_tests = lab_tests.rename(columns={"flag": "warning"})
        # convert to string, handle NaN, compare case-insensitively
        lab_tests["warning"] = (
            lab_tests["warning"]
            .fillna("")
            .astype(str)
            .str.lower()
            .apply(lambda x: 1 if x == "abnormal" else 0)
        )

    # 8. Prefix all non-ID columns with 'lab_tests_' to avoid name clashes later
    #    We keep subject_id and hadm_id as-is.
    id_cols = ["subject_id", "hadm_id"]
    cols_to_rename = [c for c in lab_tests.columns if c not in id_cols]

    rename_map = {col: "lab_tests_" + col for col in cols_to_rename}
    lab_tests = lab_tests.rename(columns=rename_map)

    # 9. Save to processed folder as Parquet
    out_path = os.path.join(HOSP_PROC_DIR, "lab_tests_clean.parquet")
    lab_tests.to_parquet(out_path, index=False)

    print(f"Saved cleaned lab tests table to: {out_path}")
    print(f"Rows: {len(lab_tests)}, Columns: {len(lab_tests.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_measurements_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))      # .../mimic_llm/scripts
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)                   # .../mimic_llm

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_DIR, ICU_PROC_DIR


def main():
    # 1. Define input paths
    chartevents_path = os.path.join(ICU_DIR, "chartevents.csv.gz")
    items_path = os.path.join(ICU_DIR, "d_items.csv.gz")

    print("Reading chartevents from:", chartevents_path)
    print("Reading d_items from:", items_path)

    # 2. Read ICU item dictionary
    items = pd.read_csv(items_path, compression="gzip")

    # Keep only rows that belong to chartevents (if linksto exists)
    if "linksto" in items.columns:
        items = items[items["linksto"] == "chartevents"].copy()

    # Ensure unique itemid in dictionary
    if "itemid" in items.columns:
        items = items.drop_duplicates(subset=["itemid"])

    # 3. Read chartevents (very large table) with selected columns only
    usecols = [
        "subject_id",
        "hadm_id",
        "stay_id",
        "itemid",
        "charttime",
        "storetime",
        "value",
        "valuenum",
        "valueuom",
        "warning",   # existing warning flag in chartevents
    ]

    chartevents = pd.read_csv(
        chartevents_path,
        compression="gzip",
        usecols=lambda c: c in usecols  # keep only these columns
    )

    print("Raw chartevents shape:", chartevents.shape)
    print("d_items (chartevents) shape:", items.shape)

    # 4. Merge to attach labels, category, unitname, etc.
    merged = chartevents.merge(
        items,
        on="itemid",
        how="left",        # keep ALL chartevents rows
        validate="m:1"     # many chartevents to 1 dictionary row
    )

    # 5. Drop unnecessary columns (column-level only, no row drops)
    cols_to_drop = [
        "storetime",
        "itemid",
        "abbreviation",
        "linksto",
        "param_type",
    ]
    existing_drop_cols = [c for c in cols_to_drop if c in merged.columns]
    measurements = merged.drop(columns=existing_drop_cols)

    # 6. Convert charttime to datetime and add date/time columns
    if "charttime" in measurements.columns:
        measurements["charttime"] = pd.to_datetime(
            measurements["charttime"], errors="coerce"
        )
        measurements["date"] = measurements["charttime"].dt.date
        measurements["time"] = measurements["charttime"].dt.time

    # 7. Prefix all non-ID columns with 'measurements_'
    id_cols = ["subject_id", "hadm_id", "stay_id"]
    cols_to_rename = [c for c in measurements.columns if c not in id_cols]

    rename_map = {col: "measurements_" + col for col in cols_to_rename}
    measurements = measurements.rename(columns=rename_map)

    # 8. Save to processed folder
    out_path = os.path.join(ICU_PROC_DIR, "measurements_clean.parquet")
    measurements.to_parquet(out_path, index=False)

    print(f"Saved cleaned measurements table to: {out_path}")
    print(f"Rows: {len(measurements)}, Columns: {len(measurements.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_medications_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_DIR, ICU_PROC_DIR


def main():
    # 1. Define input paths
    inputevents_path = os.path.join(ICU_DIR, "inputevents.csv.gz")
    items_path = os.path.join(ICU_DIR, "d_items.csv.gz")

    print("Reading inputevents from:", inputevents_path)
    print("Reading d_items from:", items_path)

    # 2. Read ICU item dictionary
    items = pd.read_csv(items_path, compression="gzip")

    # Keep only rows that belong to inputevents (if linksto exists)
    if "linksto" in items.columns:
        items = items[items["linksto"] == "inputevents"].copy()

    # Ensure unique itemid in dictionary
    if "itemid" in items.columns:
        items = items.drop_duplicates(subset=["itemid"])

    # 3. Read inputevents with selected columns
    usecols = [
        "subject_id",
        "hadm_id",
        "stay_id",
        "itemid",
        "starttime",
        "endtime",
        "amount",
        "amountuom",
        "rate",
        "rateuom",
        "ordercategoryname",
        "ordercategorydescription",
        "ordercomponenttypedescription",
        "patientweight",
        "isopenbag",
        "originalamount",
        "originalamountuom",
        "originalrate",
        "originalrateuom",
    ]

    inputevents = pd.read_csv(
        inputevents_path,
        compression="gzip",
        usecols=lambda c: c in usecols
    )

    print("Raw inputevents shape:", inputevents.shape)
    print("d_items (inputevents) shape:", items.shape)

    # 4. Merge to attach item labels, category, units, etc.
    merged = inputevents.merge(
        items,
        on="itemid",
        how="left",        # keep ALL rows
        validate="m:1"
    )

    # 5. Drop unnecessary columns
    cols_to_drop = [
        "itemid",
        "abbreviation",
        "linksto",
        "isopenbag",
        "originalamount",
        "originalamountuom",
        "originalrate",
        "originalrateuom",
        "param_type",
    ]
    existing_drop_cols = [c for c in cols_to_drop if c in merged.columns]
    medications = merged.drop(columns=existing_drop_cols)

    # 6. Convert starttime / endtime to datetime and add date/time splits
    for col in ["starttime", "endtime"]:
        if col in medications.columns:
            medications[col] = pd.to_datetime(medications[col], errors="coerce")

    if "starttime" in medications.columns:
        medications["start_date"] = medications["starttime"].dt.date
        medications["start_time"] = medications["starttime"].dt.time
    if "endtime" in medications.columns:
        medications["end_date"] = medications["endtime"].dt.date
        medications["end_time"] = medications["endtime"].dt.time

    # Optionally drop the original datetime columns if you want only derived ones
    # (okay by your "column drops allowed" rule)
    cols_to_drop2 = [c for c in ["starttime", "endtime"] if c in medications.columns]
    medications = medications.drop(columns=cols_to_drop2)

    # 7. Prefix all non-ID columns with 'medications_'
    id_cols = ["subject_id", "hadm_id", "stay_id"]
    cols_to_rename = [c for c in medications.columns if c not in id_cols]

    rename_map = {col: "medications_" + col for col in cols_to_rename}
    medications = medications.rename(columns=rename_map)

    # 8. Save to processed folder
    out_path = os.path.join(ICU_PROC_DIR, "medications_clean.parquet")
    medications.to_parquet(out_path, index=False)

    print(f"Saved cleaned medications table to: {out_path}")
    print(f"Rows: {len(medications)}, Columns: {len(medications.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_outputevents_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_DIR, ICU_PROC_DIR


def main():
    # 1. Define input paths
    outputevents_path = os.path.join(ICU_DIR, "outputevents.csv.gz")
    items_path = os.path.join(ICU_DIR, "d_items.csv.gz")

    print("Reading outputevents from:", outputevents_path)
    print("Reading d_items from:", items_path)

    # 2. Read ICU item dictionary
    items = pd.read_csv(items_path, compression="gzip")

    # Keep only rows that belong to outputevents (if linksto exists)
    if "linksto" in items.columns:
        items = items[items["linksto"] == "outputevents"].copy()

    if "itemid" in items.columns:
        items = items.drop_duplicates(subset=["itemid"])

    # 3. Read outputevents with selected columns
    usecols = [
        "subject_id",
        "hadm_id",
        "stay_id",
        "itemid",
        "charttime",
        "storetime",
        "value",
        "valueuom",
    ]

    outputevents = pd.read_csv(
        outputevents_path,
        compression="gzip",
        usecols=lambda c: c in usecols
    )

    print("Raw outputevents shape:", outputevents.shape)
    print("d_items (outputevents) shape:", items.shape)

    # 4. Merge to attach labels, normals, etc.
    merged = outputevents.merge(
        items,
        on="itemid",
        how="left",
        validate="m:1"
    )

    # 5. Compute a warning flag if we have normal ranges and numeric value
    if {"value", "lownormalvalue", "highnormalvalue"}.issubset(merged.columns):
        merged["value"] = pd.to_numeric(merged["value"], errors="coerce")
        merged["lownormalvalue"] = pd.to_numeric(merged["lownormalvalue"], errors="coerce")
        merged["highnormalvalue"] = pd.to_numeric(merged["highnormalvalue"], errors="coerce")

        merged["warning"] = 0
        mask = (
            merged["value"].notna()
            & merged["lownormalvalue"].notna()
            & merged["highnormalvalue"].notna()
        )
        merged.loc[
            mask
            & ((merged["value"] < merged["lownormalvalue"])
               | (merged["value"] > merged["highnormalvalue"])),
            "warning"
        ] = 1
    else:
        # If we don't have the needed columns, create a neutral warning column
        merged["warning"] = 0

    # 6. Drop unnecessary columns
    cols_to_drop = [
        "storetime",
        "itemid",
        "lownormalvalue",
        "highnormalvalue",
        "abbreviation",
        "linksto",
        "param_type",
    ]
    existing_drop_cols = [c for c in cols_to_drop if c in merged.columns]
    output_clean = merged.drop(columns=existing_drop_cols)

    # 7. Convert charttime to datetime and add date/time columns
    if "charttime" in output_clean.columns:
        output_clean["charttime"] = pd.to_datetime(
            output_clean["charttime"], errors="coerce"
        )
        output_clean["date"] = output_clean["charttime"].dt.date
        output_clean["time"] = output_clean["charttime"].dt.time

    # 8. Prefix all non-ID columns with 'outputevents_'
    id_cols = ["subject_id", "hadm_id", "stay_id"]
    cols_to_rename = [c for c in output_clean.columns if c not in id_cols]

    rename_map = {col: "outputevents_" + col for col in cols_to_rename}
    output_clean = output_clean.rename(columns=rename_map)

    # 9. Save to processed folder
    out_path = os.path.join(ICU_PROC_DIR, "outputevents_clean.parquet")
    output_clean.to_parquet(out_path, index=False)

    print(f"Saved cleaned outputevents table to: {out_path}")
    print(f"Rows: {len(output_clean)}, Columns: {len(output_clean.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_patients_admissions_clean.py
================================================================================
import os
import sys
import pandas as pd

# Folder where THIS script lives, e.g. /home/soham_shah/mimic_llm/scripts
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Project root = parent folder of scripts, e.g. /home/soham_shah/mimic_llm
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_DIR, HOSP_PROC_DIR

def main():
    # 1. Define input file paths
    admissions_path = os.path.join(HOSP_DIR, "admissions.csv.gz")
    patients_path = os.path.join(HOSP_DIR, "patients.csv.gz")

    print("Reading admissions from:", admissions_path)
    print("Reading patients from:", patients_path)

    # 2. Read raw tables
    admissions = pd.read_csv(admissions_path, compression="gzip")
    patients = pd.read_csv(patients_path, compression="gzip")

    # 3. Merge on subject_id (many admissions per patient)
    # validate="m:1" checks that patients has at most one row per subject_id
    df = admissions.merge(
        patients,
        on="subject_id",
        how="left",
        validate="m:1"
    )

    # 4. Convert date/time columns to proper datetimes
    datetime_cols = ["admittime", "dischtime", "deathtime", "edregtime", "edouttime"]
    for col in datetime_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce")

    # 5. Build a unified deathdate column using deathtime (admission) + dod (patients)
    #    -> first, ensure dod is datetime as well
    if "dod" in df.columns:
        df["dod"] = pd.to_datetime(df["dod"], errors="coerce")

    # start with deathtime as deathdate
    df["deathdate"] = df.get("deathtime")

    # where deathdate is missing, fill from dod
    if "dod" in df.columns:
        missing_death = df["deathdate"].isna() & df["dod"].notna()
        df.loc[missing_death, "deathdate"] = df.loc[missing_death, "dod"]

    # 6. Ensure hospital_expire_flag is 1 whenever deathdate is present
    if "hospital_expire_flag" in df.columns:
        df.loc[df["deathdate"].notna(), "hospital_expire_flag"] = 1

    # 7. Create simple date-only columns for convenience
    if "admittime" in df.columns:
        df["admit_date"] = df["admittime"].dt.date
    if "dischtime" in df.columns:
        df["discharge_date"] = df["dischtime"].dt.date

    # 8. Drop columns we don’t plan to use (can adjust later if needed)
    drop_cols = [
        "anchor_year",
        "anchor_year_group",
        "admit_provider_id",
        "insurance",
        "language",
        "marital_status",
        "race",
        "dod",          # we now have deathdate instead
    ]
    existing_drop_cols = [c for c in drop_cols if c in df.columns]
    df = df.drop(columns=existing_drop_cols)

    # 9. Save to processed folder as Parquet
    out_path = os.path.join(HOSP_PROC_DIR, "patients_admissions_clean.parquet")
    df.to_parquet(out_path, index=False)

    print(f"Saved cleaned table to: {out_path}")
    print(f"Rows: {len(df)}, Columns: {len(df.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_procedureevents_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import ICU_DIR, ICU_PROC_DIR


def main():
    # 1. Define input paths
    procedureevents_path = os.path.join(ICU_DIR, "procedureevents.csv.gz")
    items_path = os.path.join(ICU_DIR, "d_items.csv.gz")

    print("Reading procedureevents from:", procedureevents_path)
    print("Reading d_items from:", items_path)

    # 2. Read ICU item dictionary
    items = pd.read_csv(items_path, compression="gzip")

    # Keep only rows that belong to procedureevents (if linksto exists)
    if "linksto" in items.columns:
        items = items[items["linksto"] == "procedureevents"].copy()

    if "itemid" in items.columns:
        items = items.drop_duplicates(subset=["itemid"])

    # 3. Read procedureevents with selected columns
    usecols = [
        "subject_id",
        "hadm_id",
        "stay_id",
        "itemid",
        "starttime",
        "endtime",
        "storetime",
        "value",
        "valueuom",
        "location",
        "ordercategoryname",
        "ordercategorydescription",
        "ordercomponenttypedescription",
        "statusdescription",
    ]

    procedureevents = pd.read_csv(
        procedureevents_path,
        compression="gzip",
        usecols=lambda c: c in usecols
    )

    print("Raw procedureevents shape:", procedureevents.shape)
    print("d_items (procedureevents) shape:", items.shape)

    # 4. Merge to attach labels, normals, etc.
    merged = procedureevents.merge(
        items,
        on="itemid",
        how="left",
        validate="m:1"
    )

    # 5. Compute a warning flag if we have normal ranges and a numeric value
    if {"value", "lownormalvalue", "highnormalvalue"}.issubset(merged.columns):
        merged["value"] = pd.to_numeric(merged["value"], errors="coerce")
        merged["lownormalvalue"] = pd.to_numeric(merged["lownormalvalue"], errors="coerce")
        merged["highnormalvalue"] = pd.to_numeric(merged["highnormalvalue"], errors="coerce")

        merged["warning"] = 0
        mask = (
            merged["value"].notna()
            & merged["lownormalvalue"].notna()
            & merged["highnormalvalue"].notna()
        )
        merged.loc[
            mask
            & ((merged["value"] < merged["lownormalvalue"])
               | (merged["value"] > merged["highnormalvalue"])),
            "warning"
        ] = 1
    else:
        merged["warning"] = 0

    # 6. Drop unnecessary columns
    cols_to_drop = [
        "storetime",
        "itemid",
        "lownormalvalue",
        "highnormalvalue",
        "abbreviation",
        "linksto",
        "param_type",
    ]
    existing_drop_cols = [c for c in cols_to_drop if c in merged.columns]
    proc_clean = merged.drop(columns=existing_drop_cols)

    # 7. Convert starttime / endtime to datetime and add date columns
    for col in ["starttime", "endtime"]:
        if col in proc_clean.columns:
            proc_clean[col] = pd.to_datetime(proc_clean[col], errors="coerce")

    if "starttime" in proc_clean.columns:
        proc_clean["start_date"] = proc_clean["starttime"].dt.date
    if "endtime" in proc_clean.columns:
        proc_clean["end_date"] = proc_clean["endtime"].dt.date

    # Optionally drop raw datetime columns
    cols_to_drop2 = [c for c in ["starttime", "endtime"] if c in proc_clean.columns]
    proc_clean = proc_clean.drop(columns=cols_to_drop2)

    # 8. Prefix all non-ID columns with 'procedureevents_'
    id_cols = ["subject_id", "hadm_id", "stay_id"]
    cols_to_rename = [c for c in proc_clean.columns if c not in id_cols]

    rename_map = {col: "procedureevents_" + col for col in cols_to_rename}
    proc_clean = proc_clean.rename(columns=rename_map)

    # 9. Save to processed folder
    out_path = os.path.join(ICU_PROC_DIR, "procedureevents_clean.parquet")
    proc_clean.to_parquet(out_path, index=False)

    print(f"Saved cleaned procedureevents table to: {out_path}")
    print(f"Rows: {len(proc_clean)}, Columns: {len(proc_clean.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/make_procedures_clean.py
================================================================================
import os
import sys
import pandas as pd

# --- Make sure Python can see paths.py in the project root ---

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))      # .../mimic_llm/scripts
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)                   # .../mimic_llm

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from paths import HOSP_DIR, HOSP_PROC_DIR


def main():
    # 1. Define input paths for procedures and dictionary
    proc_path = os.path.join(HOSP_DIR, "procedures_icd.csv.gz")
    proc_dict_path = os.path.join(HOSP_DIR, "d_icd_procedures.csv.gz")

    print("Reading procedures from:", proc_path)
    print("Reading procedures dictionary from:", proc_dict_path)

    # 2. Read raw tables
    procedures = pd.read_csv(proc_path, compression="gzip")
    proc_dict = pd.read_csv(proc_dict_path, compression="gzip")

    # 3. Ensure dictionary has unique (icd_code, icd_version) pairs
    if {"icd_code", "icd_version"}.issubset(proc_dict.columns):
        proc_dict = proc_dict.drop_duplicates(subset=["icd_code", "icd_version"])

    # 4. Merge to attach long_title (human-readable procedure description)
    df = procedures.merge(
        proc_dict,
        on=["icd_code", "icd_version"],
        how="left",
        validate="m:1"  # many procedures rows to 1 dictionary row
    )

    # 5. Convert procedure date to datetime for easier use
    if "chartdate" in df.columns:
        df["procedure_chartdatetime"] = pd.to_datetime(df["chartdate"], errors="coerce")
        df["procedure_date"] = df["procedure_chartdatetime"].dt.date
        # Drop original chartdate so we don't have duplicates
        df = df.drop(columns=["chartdate"])

    # 6. Rename columns to make their purpose clear
    if "seq_num" in df.columns:
        df = df.rename(columns={"seq_num": "proc_seq_num"})

    if "long_title" in df.columns:
        df = df.rename(columns={"long_title": "proc_long_title"})

    # 7. Optionally drop raw code columns (we have proc_long_title now)
    cols_to_drop = [c for c in ["icd_code", "icd_version"] if c in df.columns]
    if cols_to_drop:
        df = df.drop(columns=cols_to_drop)

    # 8. Save to processed folder as Parquet
    out_path = os.path.join(HOSP_PROC_DIR, "procedures_clean.parquet")
    df.to_parquet(out_path, index=False)

    print(f"Saved cleaned procedures table to: {out_path}")
    print(f"Rows: {len(df)}, Columns: {len(df.columns)}")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/run_single_stay_inference.py
================================================================================
import os
import sys
import argparse

# wire imports from project root
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from features import (
    load_all_tables_for_stay,
    build_view_dx_proc,
    build_view_labs,
    build_view_meds,
    build_view_measurements,
    build_view_admission,    # NEW
    # build_view_final,  still available if we want it later
)
from prompts import make_prompt
from models import generate_flan, generate_meditron
from eval import compare_summaries


VALID_VIEWS = ["dx_proc", "labs", "meds", "measurements", "final", "admission"]


def pretty_header(title: str):
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="Run FLAN-T5 and Meditron summaries for a single stay_id and view."
    )
    parser.add_argument(
        "--stay_id",
        type=int,
        required=True,
        help="ICU stay_id from the 250-stay cohort.",
    )
    parser.add_argument(
        "--view_type",
        type=str,
        choices=VALID_VIEWS,
        default="final",
        help="Which view to summarise: dx_proc, labs, meds, measurements, final, admission.",
    )
    args = parser.parse_args()

    stay_id = args.stay_id
    view_type = args.view_type

    pretty_header(f"RUNNING INFERENCE FOR stay_id={stay_id}, view_type={view_type}")

    # 1. Load all tables for this stay
    stay_data = load_all_tables_for_stay(stay_id)
    hadm_id = stay_data["hadm_id"]
    subject_id = stay_data["subject_id"]
    discharge_text = stay_data["discharge_text"]

    print(f"subject_id : {subject_id}")
    print(f"hadm_id    : {hadm_id}")
    print(f"stay_id    : {stay_id}")
    print(f"view_type  : {view_type}")
    print(f"Discharge note present: {bool(discharge_text.strip())}")

    # ------------------------------------------------------------------
    # 2. Build summaries depending on view_type
    # ------------------------------------------------------------------

    # For the "final" view, we now build a multi-section composite:
    #   - dx_proc summary
    #   - labs summary
    #   - meds summary
    #   - measurements summary
    #
    # and stitch them into one big summary for each model.
    # ------------------------------------------------------------------
    if view_type == "final":
        pretty_header("GENERATING MULTI-SECTION FINAL SUMMARY...")

        # Build features for each component view
        dx_features = build_view_dx_proc(stay_data)
        labs_features = build_view_labs(stay_data)
        meds_features = build_view_meds(stay_data)
        meas_features = build_view_measurements(stay_data)

        # ----------------------- FLAN-T5 sections -----------------------
        flan_dx_prompt = make_prompt("dx_proc", dx_features, model_name="flan")
        flan_labs_prompt = make_prompt("labs", labs_features, model_name="flan")
        flan_meds_prompt = make_prompt("meds", meds_features, model_name="flan")
        flan_meas_prompt = make_prompt("measurements", meas_features, model_name="flan")

        flan_dx_summary = generate_flan(flan_dx_prompt, max_new_tokens=192)
        flan_labs_summary = generate_flan(flan_labs_prompt, max_new_tokens=192)
        flan_meds_summary = generate_flan(flan_meds_prompt, max_new_tokens=192)
        flan_meas_summary = generate_flan(flan_meas_prompt, max_new_tokens=192)

        flan_summary = (
            "Diagnosis and admission context:\n"
            + flan_dx_summary.strip() + "\n\n"
            + "Laboratory events during the ICU stay:\n"
            + flan_labs_summary.strip() + "\n\n"
            + "Medications and therapies during the ICU stay:\n"
            + flan_meds_summary.strip() + "\n\n"
            + "ICU measurements and clinical course:\n"
            + flan_meas_summary.strip()
        )

        # -------------------- Meditron-7B sections ----------------------
        med_dx_prompt = make_prompt("dx_proc", dx_features, model_name="meditron")
        med_labs_prompt = make_prompt("labs", labs_features, model_name="meditron")
        med_meds_prompt = make_prompt("meds", meds_features, model_name="meditron")
        med_meas_prompt = make_prompt("measurements", meas_features, model_name="meditron")

        med_dx_summary = generate_meditron(med_dx_prompt, max_new_tokens=192, temperature=0.0)
        med_labs_summary = generate_meditron(med_labs_prompt, max_new_tokens=192, temperature=0.0)
        med_meds_summary = generate_meditron(med_meds_prompt, max_new_tokens=192, temperature=0.0)
        med_meas_summary = generate_meditron(med_meas_prompt, max_new_tokens=192, temperature=0.0)

        meditron_summary = (
            "Diagnosis and admission context:\n"
            + med_dx_summary.strip() + "\n\n"
            + "Laboratory events during the ICU stay:\n"
            + med_labs_summary.strip() + "\n\n"
            + "Medications and therapies during the ICU stay:\n"
            + med_meds_summary.strip() + "\n\n"
            + "ICU measurements and clinical course:\n"
            + med_meas_summary.strip()
        )

    else:
        # ------------------------------------------------------------------
        # Non-final views: single prompt per model.
        #   - dx_proc
        #   - labs
        #   - meds
        #   - measurements
        #   - admission
        # ------------------------------------------------------------------
        if view_type == "dx_proc":
            features = build_view_dx_proc(stay_data)
        elif view_type == "labs":
            features = build_view_labs(stay_data)
        elif view_type == "meds":
            features = build_view_meds(stay_data)
        elif view_type == "measurements":
            features = build_view_measurements(stay_data)
        elif view_type == "admission":
            features = build_view_admission(stay_data)
        else:
            raise ValueError(f"Invalid view_type: {view_type}")

        flan_prompt = make_prompt(view_type, features, model_name="flan")
        meditron_prompt = make_prompt(view_type, features, model_name="meditron")

        pretty_header("GENERATING SUMMARIES...")

        flan_summary = generate_flan(flan_prompt, max_new_tokens=192)
        meditron_summary = generate_meditron(meditron_prompt, max_new_tokens=192, temperature=0.0)

    # ------------------------------------------------------------------
    # 3. Print summaries + discharge note + metrics
    # ------------------------------------------------------------------

    pretty_header("FLAN-T5 SUMMARY")
    print(flan_summary)
    print("\n[Length: {} tokens approx]".format(len(flan_summary.split())))

    pretty_header("MEDITRON-7B SUMMARY")
    print(meditron_summary)
    print("\n[Length: {} tokens approx]".format(len(meditron_summary.split())))

    pretty_header("ACTUAL DISCHARGE SUMMARY (FIRST 1200 CHARS)")
    if discharge_text.strip():
        snippet = discharge_text[:1200]
        print(snippet)
        if len(discharge_text) > 1200:
            print("\n[... truncated ...]")
    else:
        print("(No discharge summary available for this hadm_id in the cohort.)")

    pretty_header("EVALUATION METRICS (vs discharge summary)")

    metrics = compare_summaries(flan_summary, meditron_summary, discharge_text)

    fl = metrics["flan"]
    md = metrics["meditron"]

    print("Metric                         |   FLAN-T5   |  Meditron-7B")
    print("------------------------------+------------+-------------")
    print(f"BERTScore Precision           |  {fl['bert_precision']:.3f}    |  {md['bert_precision']:.3f}")
    print(f"Embedding similarity          |  {fl['embedding_similarity']:.3f}    |  {md['embedding_similarity']:.3f}")
    print(f"Avg sentence length (tokens)  |  {fl['avg_sentence_length']:.1f}    |  {md['avg_sentence_length']:.1f}")
    print(f"Medical term density          |  {fl['medical_term_density']:.3f}    |  {md['medical_term_density']:.3f}")

    print("\nDone.")


if __name__ == "__main__":
    main()

================================================================================
FILE: visuals.py
================================================================================
"""
visuals.py

Streamlit helpers for:
- Time-series plots from numeric ICU / HOSP tables
- Small structured tables for demographics, diagnoses, and procedures

These functions operate on the `stay_data` dict returned by
`features.load_all_tables_for_stay(stay_id)`.
"""

from typing import Dict, Any, List, Optional

import pandas as pd
import streamlit as st

# Altair is optional – if not installed, we fall back to st.line_chart.
try:
    import altair as alt  # type: ignore
except ImportError:
    alt = None


# ---------------------------------------------------------------------
# Generic helpers
# ---------------------------------------------------------------------


def _downsample(df: pd.DataFrame, max_points: int = 200) -> pd.DataFrame:
    """Thin a time-series DataFrame to at most `max_points` rows."""
    if df.empty or len(df) <= max_points:
        return df
    step = max(1, len(df) // max_points)
    return df.iloc[::step].copy()


def _combine_date_time(
    df: pd.DataFrame,
    date_col: Optional[str],
    time_col: Optional[str],
    new_col: str,
) -> pd.DataFrame:
    """Create a datetime column from date + time (if available)."""
    df = df.copy()
    if date_col and date_col in df.columns and time_col and time_col in df.columns:
        df[new_col] = pd.to_datetime(
            df[date_col].astype(str) + " " + df[time_col].astype(str),
            errors="coerce",
        )
    elif date_col and date_col in df.columns:
        df[new_col] = pd.to_datetime(df[date_col], errors="coerce")
    elif time_col and time_col in df.columns:
        df[new_col] = pd.to_datetime(df[time_col], errors="coerce")
    else:
        df[new_col] = pd.NaT
    return df


def _safe_get_table(stay_data: Dict[str, Any], group: str, name: str) -> pd.DataFrame:
    """Helper to pull a DataFrame out of stay_data['icu'] or stay_data['hosp']."""
    group_dict = stay_data.get(group, {})
    if not isinstance(group_dict, dict):
        return pd.DataFrame()
    table = group_dict.get(name)
    if table is None:
        return pd.DataFrame()
    return table.copy()


# ---------------------------------------------------------------------
# TIME-SERIES PLOTS
# ---------------------------------------------------------------------

def render_medications_visuals(
    stay_data: Dict[str, Any], max_labels: int = 5, icu_intime: pd.Timestamp = None
) -> None:
    """
    For ICU medications: Plot Amount vs Hours since admission.
    """
    meds = _safe_get_table(stay_data, "icu", "medications")
    if meds.empty:
        st.info("No ICU medication records available.")
        return

    # [Existing logic to find columns...]
    label_col = "medications_label"
    amount_col = "medications_amount"
    
    # Combined date/time logic
    meds = _combine_date_time(
        meds,
        date_col="medications_start_date" if "medications_start_date" in meds.columns else None,
        time_col="medications_start_time" if "medications_start_time" in meds.columns else None,
        new_col="med_start_dt",
    )
    meds = meds.dropna(subset=["med_start_dt", amount_col])

    if meds.empty:
        return

    # Filter labels
    label_counts = meds[label_col].value_counts()
    top_labels = label_counts.head(max_labels).index.tolist()
    all_labels = list(label_counts.index)
    selected_label = st.selectbox("Choose medication", all_labels, key="meds_select")

    df_label = meds[meds[label_col] == selected_label].copy()
    df_label = df_label.sort_values("med_start_dt")
    df_label = _downsample(df_label)

    # --- NEW X-AXIS LOGIC ---
    df_label["time"] = df_label["med_start_dt"]
    df_label["value"] = pd.to_numeric(df_label[amount_col], errors="coerce")
    
    x_axis_def = alt.X("time:T", title="Start Time")
    
    if icu_intime is not None:
        df_label["hours_since_admit"] = (df_label["time"] - icu_intime).dt.total_seconds() / 3600.0
        x_axis_def = alt.X("hours_since_admit:Q", title="Hours since ICU Admission")

    if alt is not None:
        chart = (
            alt.Chart(df_label)
            .mark_line(point=True)
            .encode(
                x=x_axis_def,
                y=alt.Y("value:Q", title="Amount"),
                tooltip=["time:T", "value:Q"]
            )
            .properties(title=f"Medication: {selected_label}", height=300)
        )
        st.altair_chart(chart, use_container_width=True)
    else:
        st.line_chart(df_label.set_index("time")["value"])


"""
def render_medications_visuals(stay_data: Dict[str, Any], max_labels: int = 5) -> None:
    
    For ICU medications:
    - Group by medications_label
    - Let user pick a label
    - Plot medications_amount over time (start datetime on x-axis)
    
    meds = _safe_get_table(stay_data, "icu", "medications")
    if meds.empty:
        st.info("No ICU medication records available for this stay.")
        return

    if "medications_label" not in meds.columns or "medications_amount" not in meds.columns:
        st.info("Medications table does not have label and amount columns.")
        return

    meds = _combine_date_time(
        meds,
        date_col="medications_start_date" if "medications_start_date" in meds.columns else None,
        time_col="medications_start_time" if "medications_start_time" in meds.columns else None,
        new_col="med_start_dt",
    )
    meds = meds.dropna(subset=["med_start_dt", "medications_amount"])
    if meds.empty:
        st.info("No medication records with valid start time and amount.")
        return

    # Choose label
    label_col = "medications_label"
    amount_col = "medications_amount"

    label_counts = meds[label_col].value_counts()
    if label_counts.empty:
        st.info("No medication labels to display.")
        return

    top_labels = label_counts.head(max_labels).index.tolist()
    all_labels = list(label_counts.index)
    default_idx = 0
    if top_labels:
        default_label = top_labels[0]
        default_idx = all_labels.index(default_label)

    selected_label = st.selectbox(
        "Choose a medication to plot",
        all_labels,
        index=default_idx,
        key="meds_label_select",
    )

    df_label = meds[meds[label_col] == selected_label].copy()
    df_label = df_label.sort_values("med_start_dt")
    df_label = _downsample(df_label)

    if df_label.empty:
        st.info(f"No records found for medication '{selected_label}'.")
        return

    df_label["time"] = df_label["med_start_dt"]
    df_label["value"] = pd.to_numeric(df_label[amount_col], errors="coerce")
    df_label = df_label.dropna(subset=["value"])

    if df_label.empty:
        st.info(f"No numeric dose values to plot for '{selected_label}'.")
        return

    if alt is not None:
        chart = (
            alt.Chart(df_label)
            .mark_line(point=True)
            .encode(
                x=alt.X("time:T", title="Start time"),
                y=alt.Y("value:Q", title="Amount"),
                tooltip=["time:T", "value:Q"],
            )
            .properties(title=f"Medication: {selected_label}", height=300)
        )
        st.altair_chart(chart, use_container_width=True)
    else:
        chart_df = df_label.set_index("time")[["value"]]
        st.line_chart(chart_df)
"""


def render_measurements_visuals(
    stay_data: Dict[str, Any], max_labels: int = 5, icu_intime: pd.Timestamp = None
) -> None:
    """
    For ICU measurements: Plot value vs Hours since admission.
    """
    meas = _safe_get_table(stay_data, "icu", "measurements")
    if meas.empty:
        st.info("No ICU measurements available.")
        return

    # [Same label/column identification logic as before...]
    # ... (Keep existing logic to find label_col, val_col, time_col) ...
    # Assuming standard names for brevity in this snippet:
    label_col = "measurements_label"
    val_col = "measurements_valuenum"
    if "measurements_charttime" in meas.columns:
        meas["meas_time"] = pd.to_datetime(meas["measurements_charttime"], errors="coerce")
    else:
        return

    # Filter labels (Same logic as before)
    label_counts = meas[label_col].value_counts()
    top_labels = label_counts.head(max_labels).index.tolist()
    all_labels = list(label_counts.index)
    selected_label = st.selectbox("Choose measurement", all_labels, key="meas_select")

    df_label = meas[meas[label_col] == selected_label].copy()
    df_label = df_label.sort_values("meas_time")
    df_label = _downsample(df_label)

    # --- NEW X-AXIS LOGIC ---
    df_label["time"] = df_label["meas_time"]
    df_label["value"] = pd.to_numeric(df_label[val_col], errors="coerce")
    
    x_axis_def = alt.X("time:T", title="Time")
    
    if icu_intime is not None:
        # Calculate hours since admission
        df_label["hours_since_admit"] = (df_label["time"] - icu_intime).dt.total_seconds() / 3600.0
        x_axis_def = alt.X("hours_since_admit:Q", title="Hours since ICU Admission")

    if alt is not None:
        chart = (
            alt.Chart(df_label)
            .mark_line(point=True)
            .encode(
                x=x_axis_def, # Use relative time if available
                y=alt.Y("value:Q", title="Value"),
                tooltip=["time:T", "value:Q", "hours_since_admit:Q"] if icu_intime else ["time:T", "value:Q"],
            )
            .properties(title=f"{selected_label}", height=300)
        )
        st.altair_chart(chart, use_container_width=True)
    else:
        st.line_chart(df_label.set_index("time")["value"])



"""
def render_measurements_visuals(
    stay_data: Dict[str, Any], max_labels: int = 5
) -> None:
    
    For ICU measurements:
    - Optionally filter to param_type == 'Numeric' if available
    - Group by measurements_label
    - Let user pick a label
    - Plot measurements_valuenum vs measurements_charttime
    
    meas = _safe_get_table(stay_data, "icu", "measurements")
    if meas.empty:
        st.info("No ICU measurements available for this stay.")
        return

    if "measurements_label" not in meas.columns or "measurements_valuenum" not in meas.columns:
        st.info("Measurements table does not have label and numeric value columns.")
        return

    # Optional filter to numeric param_type
    if "measurements_param_type" in meas.columns:
        meas = meas[meas["measurements_param_type"] == "Numeric"].copy()

    if "measurements_charttime" in meas.columns:
        meas["meas_time"] = pd.to_datetime(meas["measurements_charttime"], errors="coerce")
    else:
        st.info("Measurements table does not have a chart time column.")
        return

    meas = meas.dropna(subset=["meas_time", "measurements_valuenum"])
    if meas.empty:
        st.info("No numeric measurement values with valid timestamps.")
        return

    label_col = "measurements_label"
    value_col = "measurements_valuenum"

    label_counts = meas[label_col].value_counts()
    if label_counts.empty:
        st.info("No measurement labels to display.")
        return

    top_labels = label_counts.head(max_labels).index.tolist()
    all_labels = list(label_counts.index)
    default_idx = 0
    if top_labels:
        default_label = top_labels[0]
        default_idx = all_labels.index(default_label)

    selected_label = st.selectbox(
        "Choose a measurement to plot",
        all_labels,
        index=default_idx,
        key="meas_label_select",
    )

    df_label = meas[meas[label_col] == selected_label].copy()
    df_label = df_label.sort_values("meas_time")
    df_label = _downsample(df_label)

    if df_label.empty:
        st.info(f"No records found for measurement '{selected_label}'.")
        return

    df_label["time"] = df_label["meas_time"]
    df_label["value"] = pd.to_numeric(df_label[value_col], errors="coerce")
    df_label = df_label.dropna(subset=["value"])

    if df_label.empty:
        st.info(f"No numeric values to plot for measurement '{selected_label}'.")
        return

    if alt is not None:
        chart = (
            alt.Chart(df_label)
            .mark_line(point=True)
            .encode(
                x=alt.X("time:T", title="Time"),
                y=alt.Y("value:Q", title="Value"),
                tooltip=["time:T", "value:Q"],
            )
            .properties(title=f"Measurement: {selected_label}", height=300)
        )
        st.altair_chart(chart, use_container_width=True)
    else:
        chart_df = df_label.set_index("time")[["value"]]
        st.line_chart(chart_df)
"""

def render_outputs_visuals(stay_data: Dict[str, Any], max_labels: int = 5) -> None:
    """
    For ICU outputevents:
    - Group by outputevents_label
    - Let user pick a label
    - Plot outputevents_value over outputevents_charttime
    """
    outs = _safe_get_table(stay_data, "icu", "outputevents")
    if outs.empty:
        st.info("No ICU output events available for this stay.")
        return

    if "outputevents_label" not in outs.columns or "outputevents_value" not in outs.columns:
        st.info("Outputevents table does not have label and value columns.")
        return

    # Prefer charttime; fall back to date+time if needed
    if "outputevents_charttime" in outs.columns:
        outs["out_time"] = pd.to_datetime(outs["outputevents_charttime"], errors="coerce")
    else:
        outs = _combine_date_time(
            outs,
            date_col="outputevents_date" if "outputevents_date" in outs.columns else None,
            time_col="outputevents_time" if "outputevents_time" in outs.columns else None,
            new_col="out_time",
        )

    outs = outs.dropna(subset=["out_time", "outputevents_value"])
    if outs.empty:
        st.info("No output values with valid timestamps.")
        return

    label_col = "outputevents_label"
    value_col = "outputevents_value"

    label_counts = outs[label_col].value_counts()
    if label_counts.empty:
        st.info("No output event labels to display.")
        return

    top_labels = label_counts.head(max_labels).index.tolist()
    all_labels = list(label_counts.index)
    default_idx = 0
    if top_labels:
        default_label = top_labels[0]
        default_idx = all_labels.index(default_label)

    selected_label = st.selectbox(
        "Choose an output event to plot",
        all_labels,
        index=default_idx,
        key="outputs_label_select",
    )

    df_label = outs[outs[label_col] == selected_label].copy()
    df_label = df_label.sort_values("out_time")
    df_label = _downsample(df_label)

    if df_label.empty:
        st.info(f"No records found for output event '{selected_label}'.")
        return

    df_label["time"] = df_label["out_time"]
    df_label["value"] = pd.to_numeric(df_label[value_col], errors="coerce")
    df_label = df_label.dropna(subset=["value"])

    if df_label.empty:
        st.info(f"No numeric values to plot for output '{selected_label}'.")
        return

    if alt is not None:
        chart = (
            alt.Chart(df_label)
            .mark_line(point=True)
            .encode(
                x=alt.X("time:T", title="Time"),
                y=alt.Y("value:Q", title="Value"),
                tooltip=["time:T", "value:Q"],
            )
            .properties(title=f"Output event: {selected_label}", height=300)
        )
        st.altair_chart(chart, use_container_width=True)
    else:
        chart_df = df_label.set_index("time")[["value"]]
        st.line_chart(chart_df)


def render_labs_visuals(
    stay_data: Dict[str, Any], max_labels: int = 5, icu_intime: pd.Timestamp = None
) -> None:
    labs = _safe_get_table(stay_data, "hosp", "labs")
    if labs.empty:
        st.info("No labs available.")
        return

    # [Keep existing column finding logic]
    if "lab_tests_charttime" in labs.columns:
        labs["lab_time"] = pd.to_datetime(labs["lab_tests_charttime"], errors="coerce")
    else:
        return

    # Filter labels logic...
    label_counts = labs["lab_tests_label"].value_counts()
    all_labels = list(label_counts.index)
    selected_label = st.selectbox("Choose lab test", all_labels, key="labs_select")

    df_label = labs[labs["lab_tests_label"] == selected_label].copy()
    df_label = df_label.sort_values("lab_time")
    df_label = _downsample(df_label)

    # --- NEW X-AXIS LOGIC ---
    df_label["time"] = df_label["lab_time"]
    df_label["value"] = pd.to_numeric(df_label["lab_tests_valuenum"], errors="coerce")
    
    x_axis_def = alt.X("time:T", title="Date/Time")
    
    if icu_intime is not None:
        df_label["hours_since_admit"] = (df_label["time"] - icu_intime).dt.total_seconds() / 3600.0
        x_axis_def = alt.X("hours_since_admit:Q", title="Hours since ICU Admission")

    if alt is not None:
        base = alt.Chart(df_label).encode(
            x=x_axis_def,
            y=alt.Y("value:Q", title="Value"),
             tooltip=["time:T", "value:Q"]
        )
        line = base.mark_line()
        points = base.mark_circle(size=60).encode(
            color=alt.condition(
                alt.datum.lab_tests_warning == 1, 
                alt.value("red"), 
                alt.value("steelblue")
            )
        )
        st.altair_chart((line + points).interactive(), use_container_width=True)
    else:
        st.line_chart(df_label.set_index("time")["value"])


"""
def render_labs_visuals(stay_data: Dict[str, Any], max_labels: int = 5) -> None:
    
    For HOSP lab_tests (ICU-window, cohort version):
    - Group by lab_tests_label
    - Let user pick a label
    - Plot lab_tests_valuenum over lab_tests_date (or charttime)
    - Highlight abnormal (warning == 1) points in red if Altair is available
    
    labs = _safe_get_table(stay_data, "hosp", "labs")
    if labs.empty:
        st.info("No lab tests available for this admission / ICU stay.")
        return

    needed_cols = {"lab_tests_label", "lab_tests_valuenum"}
    if not needed_cols.issubset(set(labs.columns)):
        st.info("Lab tests table does not have required label and numeric columns.")
        return

    # Time axis: use lab_tests_date if present, else charttime
    if "lab_tests_date" in labs.columns:
        labs["lab_time"] = pd.to_datetime(labs["lab_tests_date"], errors="coerce")
    elif "lab_tests_charttime" in labs.columns:
        labs["lab_time"] = pd.to_datetime(labs["lab_tests_charttime"], errors="coerce")
    else:
        st.info("Lab tests table does not have a usable time column.")
        return

    labs["lab_tests_valuenum"] = pd.to_numeric(labs["lab_tests_valuenum"], errors="coerce")
    labs = labs.dropna(subset=["lab_time", "lab_tests_valuenum"])
    if labs.empty:
        st.info("No numeric lab values with valid timestamps.")
        return

    label_col = "lab_tests_label"
    value_col = "lab_tests_valuenum"

    label_counts = labs[label_col].value_counts()
    if label_counts.empty:
        st.info("No lab test labels to display.")
        return

    top_labels = label_counts.head(max_labels).index.tolist()
    all_labels = list(label_counts.index)
    default_idx = 0
    if top_labels:
        default_label = top_labels[0]
        default_idx = all_labels.index(default_label)

    selected_label = st.selectbox(
        "Choose a lab test to plot",
        all_labels,
        index=default_idx,
        key="labs_label_select",
    )

    df_label = labs[labs[label_col] == selected_label].copy()
    df_label = df_label.sort_values("lab_time")
    df_label = _downsample(df_label)

    if df_label.empty:
        st.info(f"No records found for lab test '{selected_label}'.")
        return

    df_label["time"] = df_label["lab_time"]
    df_label["value"] = df_label[value_col]
    df_label["is_abnormal"] = (
        df_label.get("lab_tests_warning", 0).fillna(0).astype(int)
    )

    if alt is not None:
        base = alt.Chart(df_label).encode(
            x=alt.X("time:T", title="Date"),
            y=alt.Y("value:Q", title="Value"),
        )
        line = base.mark_line()
        abnormal_points = base.transform_filter(
            alt.datum.is_abnormal == 1
        ).mark_circle(size=60, color="red")
        chart = (line + abnormal_points).properties(
            title=f"Lab test: {selected_label}", height=300
        )
        st.altair_chart(chart, use_container_width=True)
    else:
        chart_df = df_label.set_index("time")[["value"]]
        st.line_chart(chart_df)
"""

# ---------------------------------------------------------------------
# STRUCTURED TEXT TABLES
# ---------------------------------------------------------------------


def render_admission_table(stay_data: Dict[str, Any]) -> None:
    """Structured view of patients_admissions_clean_icu_250 row."""
    hosp = stay_data.get("hosp", {})
    patadm = hosp.get("patients_admissions")
    if patadm is None or patadm.empty:
        st.info("No admission row available for this stay.")
        return

    row = patadm.iloc[0]
    fields = [
        "gender",
        "anchor_age",
        "admission_type",
        "admission_location",
        "admittime",
        "dischtime",
        "discharge_location",
        "deathtime",
        "deathdate",
        "hospital_expire_flag",
    ]

    display_rows: List[Dict[str, Any]] = []
    for f in fields:
        if f in row.index:
            val = row[f]
            if pd.notna(val):
                display_rows.append({"Field": f, "Value": val})

    if not display_rows:
        st.info("No non-null admission fields to display.")
        return

    df_disp = pd.DataFrame(display_rows)
    st.table(df_disp)


def render_diagnoses_table(stay_data: Dict[str, Any]) -> None:
    """Ordered diagnoses for this hadm_id from diagnoses_clean_icu_250."""
    hosp = stay_data.get("hosp", {})
    dx = hosp.get("diagnoses")
    if dx is None or dx.empty:
        st.info("No diagnoses found for this admission.")
        return

    df_dx = dx.copy()
    if "dx_seq_num" in df_dx.columns:
        df_dx = df_dx.sort_values("dx_seq_num")

    cols = []
    if "dx_seq_num" in df_dx.columns:
        cols.append("dx_seq_num")
    if "dx_long_title" in df_dx.columns:
        cols.append("dx_long_title")

    if not cols:
        st.info("Diagnoses table does not have expected columns.")
        return

    df_disp = df_dx[cols].rename(
        columns={
            "dx_seq_num": "Sequence",
            "dx_long_title": "Diagnosis",
        }
    )
    st.table(df_disp)


def render_hosp_procedures_table(stay_data: Dict[str, Any]) -> None:
    """Ordered HOSP procedures (ICU-window filtered) for this stay/hadm."""
    hosp = stay_data.get("hosp", {})
    procs = hosp.get("procedures")
    if procs is None or procs.empty:
        st.info("No HOSP procedures for this ICU stay.")
        return

    df_p = procs.copy()
    if "proc_seq_num" in df_p.columns:
        df_p = df_p.sort_values("proc_seq_num")

    cols = []
    if "proc_seq_num" in df_p.columns:
        cols.append("proc_seq_num")
    if "proc_long_title" in df_p.columns:
        cols.append("proc_long_title")
    time_col = None
    if "procedure_chartdatetime" in df_p.columns:
        cols.append("procedure_chartdatetime")
        time_col = "procedure_chartdatetime"
    elif "procedure_date" in df_p.columns:
        cols.append("procedure_date")
        time_col = "procedure_date"

    df_disp = df_p[cols].rename(
        columns={
            "proc_seq_num": "Sequence",
            "proc_long_title": "Procedure",
            "procedure_chartdatetime": "Time",
            "procedure_date": "Date",
        }
    )
    st.table(df_disp)


def render_icu_procedureevents_table(stay_data: Dict[str, Any]) -> None:
    """ICU procedureevents (bedside procedures) for this stay."""
    icu = stay_data.get("icu", {})
    pe = icu.get("procedureevents")
    if pe is None or pe.empty:
        st.info("No ICU procedureevents found for this stay.")
        return

    df = pe.copy()
    # Build a simple start datetime from start_date if available
    df = _combine_date_time(
        df,
        date_col="procedureevents_start_date"
        if "procedureevents_start_date" in df.columns
        else None,
        time_col=None,  # cohort summary only shows start_date / end_date
        new_col="proc_start_dt",
    )

    cols = []
    if "procedureevents_category" in df.columns:
        cols.append("procedureevents_category")
    if "procedureevents_label" in df.columns:
        cols.append("procedureevents_label")
    if "procedureevents_location" in df.columns:
        cols.append("procedureevents_location")
    if "procedureevents_value" in df.columns:
        cols.append("procedureevents_value")
    if "procedureevents_valueuom" in df.columns:
        cols.append("procedureevents_valueuom")
    if "proc_start_dt" in df.columns:
        cols.append("proc_start_dt")

    df_disp = df[cols].rename(
        columns={
            "procedureevents_category": "Category",
            "procedureevents_label": "Procedure",
            "procedureevents_location": "Location",
            "procedureevents_value": "Value",
            "procedureevents_valueuom": "Unit",
            "proc_start_dt": "Start date",
        }
    ).sort_values("Start date")

    st.table(df_disp)

