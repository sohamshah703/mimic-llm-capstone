"""
eval.py

Evaluation utilities for model-generated ICU summaries.

Metrics:
- BERTScore Precision (content faithfulness vs discharge summary)
- Embedding similarity (style + content closeness)
- Average sentence length
- Medical term density (% of tokens that are medical-ish)
- ROUGE-1 (Word overlap stats)
"""

import os
import re
from typing import Dict, Set, Any

import torch
from bert_score import score as bert_score
from sentence_transformers import SentenceTransformer
from rouge_score import rouge_scorer

# Project Root for finding the vocab file
# Assuming eval.py is in /mimic_llm/, parent is Project Root
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# Note: If medical_vocab.txt is inside mimic_llm/, use CURRENT_DIR.
# If it is in CAPSTONE_Project/, use os.path.dirname(CURRENT_DIR).
# Based on your previous message, it is in mimic_llm/.
VOCAB_PATH = os.path.join(CURRENT_DIR, "medical_vocab.txt")

_embed_model = None
_rouge_scorer_obj = None
_MEDICAL_VOCAB: Set[str] = set()

# Fallback list if file is missing
_DEFAULT_TERMS = {
    "sepsis", "pneumonia", "respiratory", "failure", "intubated", 
    "hypotension", "tachycardia", "renal", "creatinine", "dialysis"
}

def _load_medical_vocab():
    global _MEDICAL_VOCAB
    if _MEDICAL_VOCAB:
        return _MEDICAL_VOCAB
    
    if os.path.exists(VOCAB_PATH):
        try:
            with open(VOCAB_PATH, "r", encoding="utf-8") as f:
                _MEDICAL_VOCAB = set(line.strip().lower() for line in f)
        except Exception as e:
            print(f"[eval] Error loading vocab: {e}")
            _MEDICAL_VOCAB = _DEFAULT_TERMS
    else:
        # Fallback to default if file generated by build_medical_vocab.py is missing
        _MEDICAL_VOCAB = _DEFAULT_TERMS
    return _MEDICAL_VOCAB

def _get_embed_model() -> SentenceTransformer:
    global _embed_model
    if _embed_model is None:
        _embed_model = SentenceTransformer(
            "sentence-transformers/all-MiniLM-L6-v2",
            device="cpu",
        )
    return _embed_model

def _get_rouge_scorer():
    global _rouge_scorer_obj
    if _rouge_scorer_obj is None:
        # ONLY ROUGE-1
        _rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
    return _rouge_scorer_obj

def bert_precision(pred: str, ref: str, lang: str = "en") -> float:
    if not pred.strip() or not ref.strip():
        return 0.0
    # BERTScore on CPU
    P, R, F1 = bert_score([pred], [ref], lang=lang, verbose=False, device="cpu")
    return float(P[0])

def embedding_similarity(pred: str, ref: str) -> float:
    if not pred.strip() or not ref.strip():
        return 0.0
    model = _get_embed_model()
    embeddings = model.encode([pred, ref], convert_to_tensor=True)
    v1, v2 = embeddings[0], embeddings[1]
    sim = torch.nn.functional.cosine_similarity(v1, v2, dim=0).item()
    return float(sim)

def avg_sentence_length(text: str) -> float:
    if not text.strip():
        return 0.0
    sentences = re.split(r"[.!?]+", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    if not sentences:
        return 0.0
    token_counts = [len(s.split()) for s in sentences if s.split()]
    if not token_counts:
        return 0.0
    return float(sum(token_counts) / len(token_counts))

def medical_term_density(text: str) -> float:
    """
    Calculates density using the large loaded vocabulary.
    """
    if not text.strip():
        return 0.0
    
    vocab = _load_medical_vocab()
    
    tokens = re.findall(r"[a-z]{3,}", text.lower()) # Ignore tiny words < 3 chars
    if not tokens:
        return 0.0
        
    med_hits = sum(1 for t in tokens if t in vocab)
    return float(med_hits / len(tokens))

def calculate_rouge(pred: str, ref: str) -> Dict[str, float]:
    """
    Calculate ROUGE-1 Only.
    """
    if not pred.strip() or not ref.strip():
        return {"rouge1": 0.0}
    
    scorer = _get_rouge_scorer()
    scores = scorer.score(ref, pred)
    
    return {
        "rouge1": scores['rouge1'].fmeasure
    }

def compare_summaries(
    flan_summary: str,
    meditron_summary: str,
    discharge_text: str,
) -> Dict[str, Dict[str, float]]:
    
    # Initialize zero metrics
    flan_metrics = {
        "bert_precision": 0.0,
        "embedding_similarity": 0.0,
        "avg_sentence_length": avg_sentence_length(flan_summary),
        "medical_term_density": medical_term_density(flan_summary),
        "rouge1": 0.0
    }
    
    meditron_metrics = {
        "bert_precision": 0.0,
        "embedding_similarity": 0.0,
        "avg_sentence_length": avg_sentence_length(meditron_summary),
        "medical_term_density": medical_term_density(meditron_summary),
        "rouge1": 0.0
    }

    if discharge_text.strip():
        # BERTScore
        flan_metrics["bert_precision"] = bert_precision(flan_summary, discharge_text)
        meditron_metrics["bert_precision"] = bert_precision(meditron_summary, discharge_text)
        
        # Embedding Sim
        flan_metrics["embedding_similarity"] = embedding_similarity(flan_summary, discharge_text)
        meditron_metrics["embedding_similarity"] = embedding_similarity(meditron_summary, discharge_text)
        
        # ROUGE-1 Only
        f_rouge = calculate_rouge(flan_summary, discharge_text)
        m_rouge = calculate_rouge(meditron_summary, discharge_text)
        
        flan_metrics.update(f_rouge)
        meditron_metrics.update(m_rouge)

    return {"flan": flan_metrics, "meditron": meditron_metrics}